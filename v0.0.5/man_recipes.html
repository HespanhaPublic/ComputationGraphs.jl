<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Recipes · ComputationGraphs</title><meta name="title" content="Recipes · ComputationGraphs"/><meta property="og:title" content="Recipes · ComputationGraphs"/><meta property="twitter:title" content="Recipes · ComputationGraphs"/><meta name="description" content="Documentation for ComputationGraphs."/><meta property="og:description" content="Documentation for ComputationGraphs."/><meta property="twitter:description" content="Documentation for ComputationGraphs."/><meta property="og:url" content="https://documenter.juliadocs.org/stable/man_recipes.html"/><meta property="twitter:url" content="https://documenter.juliadocs.org/stable/man_recipes.html"/><link rel="canonical" href="https://documenter.juliadocs.org/stable/man_recipes.html"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">ComputationGraphs</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Home</a></li><li><span class="tocitem">User manual</span><ul><li><a class="tocitem" href="man_guide.html">Basics</a></li><li><a class="tocitem" href="man_differentiation.html">Symbolic differentiation</a></li><li class="is-active"><a class="tocitem" href="man_recipes.html">Recipes</a><ul class="internal"><li><a class="tocitem" href="#Optimization-recipes"><span>Optimization recipes</span></a></li><li><a class="tocitem" href="#Neural-network-recipes"><span>Neural-network recipes</span></a></li></ul></li><li><a class="tocitem" href="man_code_generation.html">Code generation</a></li><li><a class="tocitem" href="examples.html">Examples</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="lib_representation.html">Representation of computation graphs</a></li><li><a class="tocitem" href="lib_public.html">API</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User manual</a></li><li class="is-active"><a href="man_recipes.html">Recipes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="man_recipes.html">Recipes</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/HespanhaPublic/ComputationGraphs.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/HespanhaPublic/ComputationGraphs.jl/blob/main/docs/src/man_recipes.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Recipes"><a class="docs-heading-anchor" href="#Recipes">Recipes</a><a id="Recipes-1"></a><a class="docs-heading-anchor-permalink" href="#Recipes" title="Permalink"></a></h1><p><code>ComputationGraphs</code> includes a few <em>recipes</em> to add to a computation graph the nodes needed to perform a &quot;common&quot; computation or to implement a &quot;common&quot; algorithm. These <em>recipes</em> are functions of the general form</p><pre><code class="language-julia hljs">output_nodes = recipe!(graph, input_nodes...; keyword_parameters)</code></pre><p>where</p><ul><li><code>graph</code> is a computation graph that is updated &quot;in-place&quot; by adding to it all the nodes needed for the recipe,</li><li><code>input_nodes</code> is typically a (named) tuple of nodes from the existing computational graph to which the recipe will be applied,</li><li><code>keyword_parameters</code> are specify options for the recipe, and</li><li><code>output_nodes</code> is typically a (named) tuple of nodes generated by the recipe. The recipe may add many more &quot;intermediate&quot; nodes to <code>graph</code>, but only those in <code>output_nodes</code> contain the recipe&#39;s &quot;final outputs&quot;.</li></ul><p>The functions <a href="lib_public.html#ComputationGraphs.D">D</a> and <a href="lib_public.html#ComputationGraphs.hessian">hessian</a> discussed in [Differentiation] are, in fact, recipes for which the input nodes specify the expression and variable to differentiate and a single output node that corresponds to the derivative.</p><p>Building upon these recipes, <code>ComputationGraphs</code> provides more complex recipes, e.g., to implement common optimization algorithms and to perform inference and training with neural networks.</p><h2 id="Optimization-recipes"><a class="docs-heading-anchor" href="#Optimization-recipes">Optimization recipes</a><a id="Optimization-recipes-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-recipes" title="Permalink"></a></h2><p><a href="lib_public.html#ComputationGraphs.gradDescent!">gradDescent!</a> is a recipe for performing the computations needed by the classical gradient descent algorithm to minimize a (scalar-valued) <em>loss function</em> <span>$J(\theta)$</span> by adjusting a set of <em>optimization parameters</em> <span>$\theta$</span>.</p><p class="math-container">\[    \theta^+ = \theta - \eta\, \nabla_\theta J(\theta)\]</p><p>The recipe used to implement gradient descent is called as follows:</p><pre><code class="language-julia hljs">(; next_theta, eta, gradients) = gradDescent!(graph; loss, theta)</code></pre><p>where the input parameters include</p><ul><li><code>loss</code> is a (scalar-valued) computation node that corresponds to the <em>loss function</em> <span>$J(\theta)$</span>,</li><li><code>theta</code> is a (named) tuple of variable nodes that correspond to the optimization parameters <span>$\theta$</span>,</li></ul><p>and the returned values include </p><ul><li><code>eta</code> is a variable node that can be used to set the <em>learning rate</em> <span>$\eta$</span>, </li><li><code>next_theta</code> is a (named) tuple of computation nodes that holds the value <span>$\theta^+$</span> of the optimization parameters <em>after one gradient descent</em> iteration,</li><li><code>gradients</code> is a (named) tuple of the computation nodes that hold the value of the gradients of the loss function with respect to the different variables in <code>theta</code>.</li></ul><p>To illustrate the use of this recipe, suppose that we want to minimize</p><p class="math-container">\[    J(x) = \| A\, x -b \|^2\]</p><p>with respect to <span>$x$</span>. The construct of the computation graph that accomplishes this using gradient descent, we would use:</p><pre><code class="language-julia hljs">using ComputationGraphs
graph = ComputationGraph{Float64}()
A = variable(graph, 4, 3)
x = variable(graph, 3)
b = variable(graph, 4)
loss = @add graph norm2(A*x-b)
theta = (;x,)
(; next_theta, eta, gradients) = gradDescent!(graph; loss, theta)</code></pre><p>With this graph in place, the actual optimization can be carried out as follows:</p><pre><code class="language-julia hljs">using BenchmarkTools
# Set fixed parameters
set!(graph, A, [1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0; 10.0 11.0 12.0])
set!(graph, b, [2.0, 2.0, 2.0, 2.0])
# Set learning rate
set!(graph, eta, 0.001)
# Initialize optimization parameter
set!(graph, x, [1.0, 1.0, 1.0])
println(&quot;initial loss: &quot;, get(graph,loss))
# Gradient descent loop
nIterations=100
for i in 1:nIterations
    compute!(graph, next_theta)       # compute next value of theta
    copyto!(graph, theta, next_theta) # execute update
end
println(&quot;final loss: &quot;, get(graph,loss))
bmk = @benchmark for i in 1:$nIterations
    compute!($graph, $next_theta)             # compute next value of theta
    copyto!($graph, $theta, $next_theta)      # execute update
end setup=(set!(graph, x, [1.0, 1.0, 1.0]))
println(&quot;final loss: &quot;, get(graph,loss))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">initial loss: fill(1630.0)
final loss: fill(2.1298844507367067)
final loss: fill(2.1298844507367067)
BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">56.896 μs</span></span> … <span class="sgr35">102.340 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">57.587 μs               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">58.096 μs</span></span> ± <span class="sgr32">  2.198 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

   ▃▆▇█<span class="sgr34">█</span>▆▄<span class="sgr32">▁</span>▁                                           ▂▂▂▂▁   ▂
  █████<span class="sgr34">█</span>██<span class="sgr32">█</span>██▇▅▄▃▄▄▅▇▇▆▅▄▅▄▁▃▁▃▁▁▁▃▁▃▁▁▁▁▃▁▁▁▁▃▁▃▁▁▁▃▆███████▇ █
  56.9 μs<span class="sgr90">       Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>      65.6 μs <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">0 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">0</span>.</code></pre><p>The recipe <a href="lib_public.html#ComputationGraphs.adam!">adam!</a> for the more complex Adam&#39;s algorithm can be found in <a href="examples.html#Examples">Examples</a></p><div class="admonition is-warning" id="Warning-81521b024fcbbe4"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-81521b024fcbbe4" title="Permalink"></a></header><div class="admonition-body"><p>Unlike <a href="lib_public.html#Base.get">get</a>, the function <a href="lib_public.html#Base.copyto!">copyto!</a> does not implicitly carry out any computations to make sure that the value being copied is <em>valid</em>. Because of this, we need to explicitly call <a href="lib_public.html#ComputationGraphs.compute!">compute!</a> before <a href="lib_public.html#Base.copyto!">copyto!</a>.</p></div></div><h2 id="Neural-network-recipes"><a class="docs-heading-anchor" href="#Neural-network-recipes">Neural-network recipes</a><a id="Neural-network-recipes-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network-recipes" title="Permalink"></a></h2><p><a href="lib_public.html#ComputationGraphs.denseChain!">denseChain!</a> is a recipe for performing inference and training of a dense forward neural network of form:</p><pre><code class="nohighlight hljs">    x[1]   = input
    x[2]   = activation(W[1] * x[1] + b[1])
    ...
    x[N-1] = activation(W[N-2] * x[N-2] + b[N-2])
    output = W[N-1] * x[N-1] + b[N-1]              # no activation in the last layer</code></pre><p>with training based on loss function of</p><pre><code class="nohighlight hljs">  loss = some_loss_function(output-reference)</code></pre><p>The recipe used for this is called as follows:</p><pre><code class="language-julia hljs">(; inference, training, theta) = denseChain!(graph; 
        nNodes, inferenceBatchSize, trainingBatchSize, activation,loss)</code></pre><p>where the input parameters include</p><ul><li><code>nNodes::Vector{Int}</code>: Vector with the number of nodes in each layer, starting from the       input and ending at the output layer.</li><li><code>inferenceBatchSize::Int=1</code>: Number of inputs for each inference batch.       When <code>inferenceBatchSize=0</code> no nodes will be created for inference.</li><li><code>trainingBatchSize::Int=0</code>: Number of inputs for each training batch.        When <code>trainingBatchSize=0</code> no nodes will be created for training.</li><li><code>activation::Function=ComputationGraphs.relu</code>: Activation function.        Use the <code>identity</code> function if no activation is desired.</li><li><code>loss::Symbol=:mse</code>: Desired type of loss function, among the options:       + :sse = sum of square error       + :mse = mean square error (i.e., sse normalized by the error size)       + :huber = huber function on the error       + :mhuber = huber function on the error, normalized by the error size</li></ul><p>and the returned tuple includes</p><ul><li><code>inference::NamedTuple</code>: named tuple with the inference nodes:       + <code>input</code> NN input for inference       + <code>output</code> NN output for inference       When <code>inferenceBatchSize=0</code> this tuple is returned empty</li></ul><p><code>+ training::NamedTuple</code>: named tuple with the training nodes:         + <code>input</code> NN input for training         + <code>output</code> NN output for training         + <code>reference</code> NN desired output for training         + <code>loss</code> NN loss for training         When <code>trainingBatchSize=0</code> this tuple is returned empty</p><ul><li><code>theta::NamedTuple</code>: named tuple with the NN parameters (all the matrices W and b)</li></ul><p>To illustrate the use of this recipe, suppose that we want to construct a neural network whose input is an angle in the [0,2*pi] range with two outputs that return the sine and cosine of the angle.</p><p>To accomplish this, will use a network with 1 input, 2 output, a few hidden layers, and <code>relu</code> activation functions.</p><p>We want the computation graph to support:</p><ul><li><em>inference</em>, i.e., compute the output for a given input;</li><li><em>training</em>, i.e., minimize the loss for a given set of inputs and desired outputs.</li></ul><p>For training we will use a large batch size, but for inference we will only provide one input at a time.</p><p>We start by using <a href="lib_public.html#ComputationGraphs.denseChain!">denseChain!</a> to construct a graph that performs all the computations needed to do inference and compute the (training) loss function for the network.</p><pre><code class="language-julia hljs">using ComputationGraphs, Random
graph=ComputationGraph{Float32}()
hiddenLayers=[20,20,20]
(; inference, training, theta)=denseChain!(graph;
        nNodes=[1,hiddenLayers...,2], inferenceBatchSize=1, trainingBatchSize=3000,
        activation=ComputationGraphs.relu, loss=:mse)</code></pre><p>We now generate random (but repeatable) training data:</p><pre><code class="language-julia hljs">Random.seed!(0)
input=(2*pi).*rand(Float32,size(training.input))
reference=vcat(sin.(input),cos.(input))
set!(graph,training.input,input)
set!(graph,training.reference,reference)</code></pre><p>Not surprisingly, with a random initialization of the network weights, the loss is large:</p><pre><code class="language-julia hljs">Random.seed!(0)
for k in eachindex(theta)
    set!(graph,theta[k],randn(Float32,size(theta[k])))
end
loss=get(graph,training.loss)
println(&quot;loss = &quot;,loss)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss = fill(198.47896f0)</code></pre><p>However, we can use an optimization recipe –- like <a href="lib_public.html#ComputationGraphs.gradDescent!">gradDescent!</a> or <a href="lib_public.html#ComputationGraphs.adam!">adam!</a> –- to train the network. This can be found in the example <a href="examples.html#Neural-network-training">Neural network training</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="man_differentiation.html">« Symbolic differentiation</a><a class="docs-footer-nextpage" href="man_code_generation.html">Code generation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Wednesday 10 September 2025 21:29">Wednesday 10 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
