var documenterSearchIndex = {"docs":
[{"location":"examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"Examples to illustrate the use of ComputationGraphs.","category":"page"},{"location":"examples.html#Contents","page":"Examples","title":"Contents","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"Pages = [\"examples.md\"]\nDepth = 2:3","category":"page"},{"location":"examples.html#Adam's-method-for-optimization","page":"Examples","title":"Adam's method for optimization","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"Adam's gradient-based optimization can be easily implement using the recipe adam! which includes all the \"messy\" formulas. In this example, we use Adam's method to minimize a quadratic criterion of the form","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"    J(x) =  A x -b ^2","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"with respect to x. To construct of the computation graph that accomplishes this, we use:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"using ComputationGraphs\ngraph = ComputationGraph{Float64}()\nA = variable(graph, 400, 300)\nx = variable(graph, 300)\nb = variable(graph, 400)\nloss = @add graph norm2(times(A, x) - b)\ntheta = (;x,)\n(;  eta, beta1, beta2, epsilon,\n    init_state, state, next_state,\n    next_theta, gradients) = adam!(graph; loss, theta)\nnothing # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"With this graph in place, the actual optimization can be carried out as follows:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We start by initializing the Adam's parameters","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"set!(graph, eta, 2e-2)\nset!(graph, beta1, 0.9)\nset!(graph, beta2, 0.999)\nset!(graph, epsilon, 1e-8)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We (randomly) initialize the problem data (freeing the random see for repeatability)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"using Random\nRandom.seed!(0)\nset!(graph, A, randn(size(A)))\nset!(graph, b, randn(size(b)))","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We then (randomly) initialize the parameters to optimize (freeing the random see for repeatability)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"Random.seed!(0)\ninit_x=randn(Float64,size(x))\nset!(graph, x, init_x)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We then initialize Adam's internal state","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"copyto!(graph, state, init_state)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We are now ready to run Adam's iterations:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"using BenchmarkTools, Plots\nl=get(graph,loss)\nprintln(\"initial loss: \", l)\nall=(;state...,theta...)\nnext_all=(;next_state...,next_theta...)\nnIterations=1000\nlosses=Vector{Float64}(undef,nIterations)\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 3 # hide\nbmk = @benchmark for i in 1:$nIterations\n    compute!($graph, $next_all)\n    copyto!($graph, $all, $next_all)\n    $l=get($graph, $loss)\n    $losses[i]=$l[1]\nend  setup =( # reinitialize x and solver for each new sample\n        set!($graph, $x, $init_x), copyto!($graph, $state, $init_state)\n    ) evals=1 # a single evaluation per sample\nprintln(\"final loss: \", l)\nplt=Plots.plot(losses,yaxis=:log,ylabel=\"loss\",xlabel=\"iteration\",label=\"\",size=(750,400))\ndisplay(plt)                            # hide\nsavefig(plt,\"example1.png\");nothing # hide\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true)) # hide\n@assert bmk.allocs==0                                       # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"As expected for a convex optimization, convergence is pretty smooth:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"(Image: convergence plot)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"note: Note\nFor @benchmark to reflect the time an actual optimization, we reset the optimization variable x and the solver's state at the start of each sample (using @benchmark's setup code).","category":"page"},{"location":"examples.html#Adam's-method-with-projection","page":"Examples","title":"Adam's method with projection","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"Suppose now that we wanted to add a \"projection\" to Adam's method to keep all entries of x positive. This could be done by simply modifying the next_theta produced by Adam to force all the entries of next_step.x to be positive, using the relu function:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"next_theta = (x=relu(graph,next_theta.x),)\nnothing # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We can now repeat the previous steps (reinitializing everything for a fresh start):","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"set!(graph, x, init_x)\ncopyto!(graph, state, init_state)\nl=get(graph,loss)\nprintln(\"initial loss: \", l)\nall=(;state...,theta...)\nnext_all=(;next_state...,next_theta...)\nnIterations=1000\nlosses=Vector{Float64}(undef,nIterations)\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 3 # hide\nbmk = @benchmark for i in 1:$nIterations\n    compute!($graph, $next_all)\n    copyto!($graph, $all, $next_all)\n    $l=get($graph,$loss)\n    $losses[i]=$l[1]\nend  setup =( # reinitialize x and solver for each new sample\n        set!($graph, $x, $init_x), copyto!($graph, $state, $init_state)\n    ) evals=1 # a single evaluation per sample\nprintln(\"final loss: \", l)\nplt=Plots.plot(losses,yaxis=:log,ylabel=\"loss\",xlabel=\"iteration\",label=\"\",size=(750,400))\ndisplay(plt)                        # hide\nsavefig(\"example2.png\");nothing # hide\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true)) # hide\n@assert bmk.allocs==0                                       # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"(Image: convergence plot)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"note: Note\nFor @benchmark to reflect the time an actual optimization, we reset the optimization variable x and the solver's state at the start of each sample (using @benchmark's setup code).","category":"page"},{"location":"examples.html#Neural-network-training","page":"Examples","title":"Neural network training","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"In this example, we combine the two recipes denseChain! and adam! to train and query a dense forward neural network of form:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"    x[1]   = input\n    x[2]   = activation(W[1] * x[1] + b[1])\n    ...\n    x[N-1] = activation(W[N-2] * x[N-2] + b[N-2])\n    output = W[N-1] * x[N-1] + b[N-1]               # no activation in the last layer\n    loss = some_loss_function(output-reference)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"As in Neural-network recipes, our goal is to train a neural network whose input is an angle in the [0,2*pi] range with two outputs that return the sine and cosine of the angle. To accomplish this will use a network with 1 input, 2 output, a few hidden layers, and relu activation functions.","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We start by using denseChain! to construct a graph that performs all the computations needed","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"to do inference and compute the (training) loss function for the network. The computation graph will support:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"inference, i.e., compute the output for a given input;\ntraining, i.e., minimize the loss for a given set of inputs and desired outputs. For training","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"we will use a large batch size, but for inference we will only provide one input at a time.","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"using ComputationGraphs, Random\ngraph=ComputationGraph{Float32}()\nhiddenLayers=[30,20,30]\n(; inference, training, theta)=denseChain!(graph; \n        nNodes=[1,hiddenLayers...,2], \n        inferenceBatchSize=1, \n        trainingBatchSize=5_000,\n        activation=ComputationGraphs.relu, \n        loss=:mse)\nprintln(\"graph with \", length(graph), \" nodes and \",ComputationGraphs.memory(graph),\" bytes\")\nnothing # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"where\n\n+ `nNodes` is a vector with the number of nodes in each layer, starting from the\n        input and ending at the output layer.\n+ `inferenceBatchSize` is the number of inputs for each inference batch.\n+ `trainingBatchSize` is the number of inputs for each training batch.\n+ `activation`: is the activation function.\n+ `loss` defines the loss to be the mean square error.\n\nand the returned tuple includes\n\n+ `inference::NamedTuple`: named tuple with the inference nodes:\n        + `input` NN input for inference\n        + `output` NN output for inference\n\n`+ training::NamedTuple`: named tuple with the training nodes:\n        + `input` NN input for training\n        + `output` NN output for training\n        + `reference` NN desired output for training\n        + `loss` NN loss for training\n\n+ `theta::NamedTuple`: named tuple with the NN parameters (all the matrices W and b)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We then use the adam! recipe add to the graph the computation needed to optimize the weights.","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"(;  eta, beta1, beta2, epsilon,\n    init_state, state, next_state,\n    next_theta, gradients) = adam!(graph; loss=training.loss, theta=theta)\nprintln(\"graph with \", length(graph), \" nodes and \",ComputationGraphs.memory(graph),\" bytes\")\nnothing # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"where we passed to [adam!](@ref) the nodes that correspond to the neural network loss and use\nthe neural network parameters as the optimization variables.\n\nIn return, we get back the nodes with Adam's parameters as well as the nodes needed for the\nalgorithm's update.","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We initialize the network weights with random (but repeatable) values. We use a function for this, to be able to call it many times.","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"function init_weights(graph,theta)\n    Random.seed!(0)\n    for k in eachindex(theta)\n        set!(graph,theta[k],0.2*randn(Float32,size(theta[k])))\n    end\nend\nnothing # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We are almost ready to use Adam's iterative algorithm, similarly to what was done in Adam's method for optimization. However, and as commonly done in training neural networks, we will use a different random set of training data at each iteration.\nTo this effect, we create a \"data-loader\" function that will create a new set of data at each iteration:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"function dataLoader!(input,output)\n    for k in eachindex(input)\n        input[k]=(2*pi)*rand(Float32)\n        output[1,k]=sin(input[k])\n        output[2,k]=cos(input[k])\n    end\nend\nnothing # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"Now we are indeed ready for training:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"using BenchmarkTools, Plots\n# Initialize Adam's parameters\nset!(graph, eta, 8e-4)\nset!(graph, beta1, 0.9)\nset!(graph, beta2, 0.999)\nset!(graph, epsilon, 1e-8)\n# create arrays for batch data\ninput=Array{Float32}(undef,size(training.input))\noutput=Array{Float32}(undef,size(training.reference))\n# Create array to save losses\nnIterations=1_000\nlosses=Vector{Float32}(undef,nIterations)\n# Adam iteration\nall=(;state...,theta...)\nnext_all=(;next_state...,next_theta...)\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 10 # hide\nbmk = BenchmarkTools.@benchmark for i in 1:$nIterations\n    dataLoader!($input,$output)\n    set!($graph,$training.input,$input)\n    set!($graph,$training.reference,$output)\n    compute!($graph, $next_all)\n    copyto!($graph, $all, $next_all)\n    l=get($graph, $training.loss)\n    $losses[i]=l[1]\nend  setup =( # reinitialize NN weights and solver for each new sample\n        init_weights($graph,$theta), copyto!($graph, $state, $init_state)\n    ) evals=1 # a single evaluation per sample\nprintln(\"final loss: \", get(graph,training.loss))\nplt=Plots.plot(losses,yaxis=:log,\n    ylabel=\"loss\",xlabel=\"iteration\",label=\"\",size=(750,400))\ndisplay(plt)                            # hide\nsavefig(\"example3a.png\");nothing    # hide\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true)) # hide\n#@assert bmk.allocs==0      # FIXME getting some small allocations ???                                 # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"In spite of not being a convex optimization, convergence is still pretty good (after carefully choosing the step size eta).","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"(Image: convergence plot)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We can now check how the neural network is doing at computing the sine and cosine:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"angles=0:.01:2*pi\noutputs=Array{Float32}(undef,2,length(angles))\nfor (k,angle) in enumerate(angles)\n    set!(graph,inference.input,[angle])\n    (outputs[1,k],outputs[2,k])=get(graph,inference.output)\nend\nplt=Plots.plot(angles,outputs',\n    xlabel=\"angle\",ylabel=\"outputs\",label=[\"sin\" \"cos\"],size=(750,400))\ndisplay(plt)                         # hide\nsavefig(\"example3b.png\");nothing      # hide","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"and it looks like the network is doing quite well at computing the sine and cosine:","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"(Image: inference)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"warning: Warning\nThe code above does inference one angle at a time, which is quite inefficient. This could be avoided, by setting inferenceBatchSize to a value larger than 1.","category":"page"},{"location":"man_code_generation.html#Code-generation","page":"Code generation","title":"Code generation","text":"","category":"section"},{"location":"man_code_generation.html","page":"Code generation","title":"Code generation","text":"warning: Warning\nTo be added.","category":"page"},{"location":"lib_representation.html#Representation-of-computation-graphs","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"This section of the manual documents the inner workings of the graph computation functions in the source file src/compute.jl.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"warning: Warning\nThe timing information reported here is obtained by running Documenter's @example triggered by a Github action. Because of this, there is no control over the hardware used and consequently the timing values that appear in this document are not very reliable; especially in what regards Parallel computations.","category":"page"},{"location":"lib_representation.html#Computation-nodes","page":"Representation of computation graphs","title":"Computation nodes","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"An expression like A*x+b contains","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"three nodes A, b, and x that corresponds to variables, and\ntwo computation nodes, one for the multiplication and the other for the addition.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Since we are aiming for allocation-free computation, we start by pre-allocating memory for all nodes","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"using Random\nbegin #hide\nA = rand(Float64,400,30)  # pre-allocated storage for the variable A\nx = rand(Float64,30)     # pre-allocated storage for the variable b\nb = rand(Float64,400)     # pre-allocated storage for the variable x\nAx = similar(b)          # pre-allocated storage for the computation node A*x\nAxb = similar(b)         # pre-allocated storage for the computation node A*x+b\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"and associate to the following functions to the two computation nodes:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"using LinearAlgebra\nbegin # hide\nfunction node_Ax!(out::Vector{F},in1::Matrix{F}, in2::Vector{F}) where {F} \n    mul!(out,in1,in2)\nend\nfunction node_Axb!(out::Vector{F},in1::Vector{F}, in2::Vector{F}) where {F} \n    @. out = in1 + in2\nend\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"It would be temping to construct the computation graph out of such functions. However, every function in julia has is own unique type (all subtypes of the Function abstract type). This is problematic because we will often need to iterate over the nodes of a graph, e.g., to re-evaluate all nodes in the graph or just the parents of a specific node. If all nodes have a unique type, then such iterations not be type-stable.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"To resolve this issue we do two \"semantic\" transformations to the functions above: function closure and function wrapping with the package FunctionWrappers.","category":"page"},{"location":"lib_representation.html#Function-closure","page":"Representation of computation graphs","title":"Function closure","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Function closure allow us to obtain functions for all the nodes that \"look the same\" in the following sense:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"they all have they have the same signature (i.e., same number of input parameters and with the   same types), and\nthey all return a value of the same type.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Specifically, we \"capture\" the input parameters for the two computation nodes, which makes them look like parameter-free functions that return nothing:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\n@inline node_Ax_closed!() = let  Ax=Ax , A=A, x=x\n    node_Ax!(Ax,A,x)\n    nothing\n    end\n@inline node_Axb_closed!() = let Axb=Axb, Ax=Ax, b=b\n    node_Axb!(Axb,Ax,b)\n    nothing\n    end\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"note: Note\nSee Performance tips on the performance of captured variables on the use of let, which essentially helps the parser by \"fixing\" the captured variable. To be precise \"fixing\" the arrays, but not the values of their entries.","category":"page"},{"location":"lib_representation.html#Function-wrapping","page":"Representation of computation graphs","title":"Function wrapping","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Even though all node functions now have similar inputs and outputs, they are still not of the same type (as far as julia is concerned). To fix this issue, we use the package FunctionWrappers to create a type-stable wrapper:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nimport ComputationGraphs\nnode_Ax_wrapped = ComputationGraphs.FunctionWrapper(node_Ax_closed!)\nnode_Axb_wrapped = ComputationGraphs.FunctionWrapper(node_Axb_closed!)\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"The \"wrapped\" functions can be called directly with:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nnode_Ax_wrapped()\nnode_Axb_wrapped()\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"or a little faster with","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nComputationGraphs.do_ccall(node_Ax_wrapped)\nComputationGraphs.do_ccall(node_Axb_wrapped)\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"warning: Warning\nThe code above does not actually use FunctionWrappers; instead it uses a very simplified version of FunctionWrappers that can only wrap functions with no arguments that always return nothing.To use FunctionWrappers, we would have used insteadimport FunctionWrappers\nnode_Ax_wrapped_FW = FunctionWrappers.FunctionWrapper{Nothing,Tuple{}}(node_Ax_closed!)\nnode_Axb_wrapped_FW = FunctionWrappers.FunctionWrapper{Nothing,Tuple{}}(node_Axb_closed!)and the functions would be called withbegin # hide\nFunctionWrappers.do_ccall(node_Ax_wrapped_FW, ())\nFunctionWrappers.do_ccall(node_Axb_wrapped_FW, ())\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html#Verification","page":"Representation of computation graphs","title":"Verification","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"We can now check the fruits of our work.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Type stability?","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nprintln(\"Type stability for original: \", typeof(node_Ax!)==typeof(node_Axb!))\nprintln(\"Type stability for wrapped : \", typeof(node_Ax_wrapped)==typeof(node_Axb_wrapped))\nnothing # hide\nend #hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Correctness?","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nrand!(A)\nrand!(b)\nrand!(x)\n\n# the original functions\nnode_Ax!(Ax,A,x)\nnode_Axb!(Axb,Ax,b)\nprintln(\"Correctness for original: \", Axb==(A*x+b))\n\nrand!(A)\nrand!(b)\nrand!(x)\n\n# the new functions\nComputationGraphs.do_ccall(node_Ax_wrapped)\nComputationGraphs.do_ccall(node_Axb_wrapped)\nprintln(\"Correctness for wrapped : \", Axb==(A*x+b))\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Speed?","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"using BenchmarkTools, Printf\nbegin # hide\n@show Threads.nthreads()\nBLAS.set_num_threads(1)\n@show BLAS.get_num_threads()\n@show Base.JLOptions().opt_level\n\nbmk1 = @benchmark begin\n    node_Ax!($Ax,$A,$x)\n    node_Axb!($Axb,$Ax,$b)\nend evals=1000 samples=10000\nprintln(\"Original:\\n\",sprint(show,\"text/plain\",bmk1;context=:color=>true)) # hide\n\nbmk3 = @benchmark begin\n    node_Ax_closed!()\n    node_Axb_closed!()\nend evals=1000 samples=10000\nprintln(\"Closure:\\n\",sprint(show,\"text/plain\",bmk3;context=:color=>true)) # hide\n\nbmk2 = @benchmark begin\n    ComputationGraphs.do_ccall($node_Ax_wrapped)\n    ComputationGraphs.do_ccall($node_Axb_wrapped)\nend evals=1000 samples=10000\nprintln(\"Wrapped:\\n\",sprint(show,\"text/plain\",bmk2;context=:color=>true)) # hide\n\n@printf(\"Overhead due to closure  = %3.f ns\\n\",median(bmk3.times)-median(bmk1.times))\n@printf(\"Overhead due to wrapping = %3.f ns\\n\",median(bmk2.times)-median(bmk3.times))\n@printf(\"Total overhead           = %3.f ns\\n\",median(bmk2.times)-median(bmk1.times))\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"This shows that closure and wrapping do introduce a small overhead (tens of ns). However, the benefits of type stability will appear when we start iterating over nodes. To see this consider the following function that evaluates a set of nodes:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nfunction compute_all!(nodes::Vector{Function})\n    for node in nodes\n        node()\n    end\nend\nfunction compute_all_wrapped!(nodes::Vector{ComputationGraphs.FunctionWrapper})\n    for node::ComputationGraphs.FunctionWrapper in nodes\n        ComputationGraphs.do_ccall(node)\n    end\nend\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"We can use @code_warntype to see how wrapping helps in terms of type stability:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"using InteractiveUtils # hide\nbegin # hide\n# using just closure\nnodes_closed=repeat([node_Ax_closed!,node_Axb_closed!],outer=5)\n@show typeof(nodes_closed)\nInteractiveUtils.@code_warntype compute_all!(nodes_closed)\n#println(sprint(code_warntype,compute_all!,(typeof(nodes_closed),);context=:color=>true)) # hide\n\n# using closure+wrapped\nnodes_wrapped=repeat([node_Ax_wrapped,node_Axb_wrapped],outer=5)\n@show typeof(nodes_wrapped)\nInteractiveUtils.@code_warntype compute_all_wrapped!(nodes_wrapped)\n#println(sprint(code_warntype,compute_all_wrapped!,(typeof(nodes_wrapped),);context=:color=>true)) # hide\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"These specific functions compute_all! and compute_all_wrapped! are so simple that type instability actually does not lead to heap allocations, but the use of wrapped functions still leads to slightly faster code.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\n@show typeof(nodes_closed)\nbmk3 = @benchmark compute_all!($nodes_closed) evals=1 samples=10000\nprintln(\"Closure:\\n\",sprint(show,\"text/plain\",bmk3;context=:color=>true)) # hide\n\n@show typeof(nodes_wrapped)\nbmk2 = @benchmark compute_all_wrapped!($nodes_wrapped)  evals=1 samples=10000\nprintln(\"Closure+Wrapping:\\n\",sprint(show,\"text/plain\",bmk2;context=:color=>true)) # hide\n\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html#Conditional-computations","page":"Representation of computation graphs","title":"Conditional computations","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"So far we discussed how to compute all nodes or some give vector of nodes. Restricting evaluations to just the set of nodes that need to be recomputed requires introducing some simple logic to the function closures.","category":"page"},{"location":"lib_representation.html#Implementation","page":"Representation of computation graphs","title":"Implementation","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"To support need-based evaluations, we use a BitVector to keep track of which nodes have been evaluated. For our 2-node example, we would use","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"validValue=falses(2)\nnothing # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"The functions below now include the logic for need-based evaluation:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nnode_Ax_conditional_closed() = let validValue=validValue, \n    Ax=Ax , A=A, x=x\n    node_Ax!(Ax,A,x)    # this node's computation\n    nothing\n    end\nnode_Ax_conditional_wrapped = ComputationGraphs.FunctionWrapper(node_Ax_conditional_closed)\nnode_Axb_conditional_closed() = let validValue=validValue, \n    Axb=Axb, Ax=Ax, b=b, \n    node_Ax_conditional_wrapped=node_Ax_conditional_wrapped\n    # compute parent node Ax (if needed)\n    if !validValue[1]\n        validValue[1]=true\n         ComputationGraphs.do_ccall(node_Ax_conditional_wrapped)\n    end\n    node_Axb!(Axb,Ax,b)  # this nodes' computation\n    nothing\n    end\nnode_Axb_conditional_wrapped = ComputationGraphs.FunctionWrapper(node_Axb_conditional_closed)\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"With this logic, we only need a call to evaluate the node A*x+b, as this will automatically trigger the evaluation of A*x (if needed). To check that the logic is working, we do:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nfill!(validValue,false)\nfill!(Ax,0.0)\nfill!(Axb,0.0)\nComputationGraphs.do_ccall(node_Ax_conditional_wrapped)\n@assert validValue == [false,false] \"no parent computed\"\n@assert all(Ax .== A*x)  \"should only compute Ax\"\n@assert all(Axb .== 0) \"should only compute Ax\"\n\nfill!(validValue,false)\nfill!(Ax,0.0)\nfill!(Axb,0.0)\nComputationGraphs.do_ccall(node_Axb_conditional_wrapped)\n@assert validValue == [true,false] \"parent should have been computed\"\n@assert all(Ax .== A*x)  \"should compute both\"\n@assert all(Axb .== A*x+b) \"should compute both\"\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html#Timing-verification","page":"Representation of computation graphs","title":"Timing verification","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"We can now check the impact of the new logic on timing.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nusing BenchmarkTools, Printf\n@show Threads.nthreads()\nBLAS.set_num_threads(1)\n@show BLAS.get_num_threads()\n@show Base.JLOptions().opt_level\n\nbmk1 = @benchmark begin\n    node_Ax!($Ax,$A,$x)\n    node_Axb!($Axb,$Ax,$b)\nend evals=1000 samples=10000\nprintln(\"Unconditional computation:\\n\",sprint(show,\"text/plain\",bmk1;context=:color=>true)) # hide\n\nbmk2a = @benchmark begin\n    ComputationGraphs.do_ccall($node_Ax_wrapped)\n    ComputationGraphs.do_ccall($node_Axb_wrapped)\nend evals=1000 samples=10000\nprintln(\"Unconditional computation with wrapping:\\n\",sprint(show,\"text/plain\",bmk2;context=:color=>true)) # hide\n\nbmk2b = @benchmark begin\n    $validValue[1]=false\n    $validValue[2]=false\n    if !$validValue[2]\n        $validValue[2]=true\n        ComputationGraphs.do_ccall($node_Axb_conditional_wrapped)\n    end\nend evals=1000 samples=10000\nprintln(\"Conditional computation, but with all valid=false:\\n\",sprint(show,\"text/plain\",bmk2;context=:color=>true)) # hide\n\nbmk3 = @benchmark begin\n    if !$validValue[2]\n        $validValue[2]=true\n        ComputationGraphs.do_ccall($node_Axb_conditional_wrapped)\n    end\nend evals=1 samples=10000\nprintln(\"Conditional computation, with full reuse:\\n\",sprint(show,\"text/plain\",bmk3;context=:color=>true)) # hide\n\nbmk4 = @benchmark begin\n    $validValue[2]=false\n    if !$validValue[2]\n        $validValue[2]=true\n        ComputationGraphs.do_ccall($node_Axb_conditional_wrapped)\n    end\nend evals=1000 samples=10000\nprintln(\"Conditional computation, with valid=false only for Axb:\\n\",sprint(show,\"text/plain\",bmk4;context=:color=>true)) # hide\n\n@printf(\"overhead due to closure+wrapping for full computations          = %+6.f ns\\n\",\n    median(bmk2a.times)-median(bmk1.times))\n@printf(\"overhead due to closure+wrapping+logic for full computations    = %+6.f ns\\n\",\n    median(bmk2b.times)-median(bmk1.times))\n# @printf(\"overhead due just to             logic for full computations    = %+6.f ns\\n\", # hide \n#      median(bmk2b.times)-median(bmk2a.times)) # hide\n@printf(\"overhead due to closure+wrapping+logic for for computations     = %+6.f ns (<0 means savings)\\n\",\n    median(bmk3.times)-median(bmk1.times))\n@printf(\"overhead due to closure+wrapping+logic for partial computations = %+6.f ns (<0 means savings)\\n\",\n    median(bmk4.times)-median(bmk1.times))\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"As expected, much time is saved when re-evaluations are not needed. When they are needed, the logic adds a small additional penalty.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"note: Note\nThe code above is the basis for ComputationGraphs.generateComputeFunctions.","category":"page"},{"location":"lib_representation.html#Parallel-computations","page":"Representation of computation graphs","title":"Parallel computations","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Parallel evaluation are implemented by associating to each computation node one Threads.Task and one pair of Threads.Events. For each computation node i:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"The task task[i]::Threads.Task is responsible carrying out the evaluation of node i and synchronizing it with the other nodes.\nThe event request[i]::Threads.Event(autoreset=true) is used to request task[i] to evaluate its node, by issuing notify(request[i]).\nThe event valid[i]::Threads.Event(autoreset=false) is used by node i to notify all other nodes that it has finished handling a computation request received through request[i]","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"The following protocol is used:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"All node tasks are spawn simultaneously and each task i immediately waits on request[i] for evaluation request.\nUpon receiving a request, task i checks which of its parents have valid data:\nFor every parent p with missing data, it issues an evaluation request using notify(request[p]).\nAfter that, the task waits on the requests to be fulfilled by using wait(valid[p]) for the same set of parent node.\nOnce all parents have valid data, node i performs its own computation and notifies any waiting child node that its data became valid using notify[valid[i]].","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"The operation described above makes the following assumptions:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"Any thread that needs the value of node i should first issues an evaluation request using notify(request[i]) and then wait for its completion using wait(valid[i]).\nWhen the value of a variable v changes, all its children nodes c need to be notified that their values become invalid by issuing reset(valid[c]).\nTo avoid races, these last reset(valid[c]) cannot be done while computations are being performed.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"warning: Warning\nThe last assumption above should be enforced by an explicit locking mechanism, but that has not yet been implemented.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"warning: Warning\nFor very large matrix multiplications, BLAS makes good use of multiple threads. In this case, we should not expect significant improvements with respect to evaluating the computation graph sequentially. Instead, it is better to allow BLAS to manage all the threads, with a sequential evaluation of the computation graph.","category":"page"},{"location":"lib_representation.html#Parallelism-implementation","page":"Representation of computation graphs","title":"Parallelism implementation","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"We will illustrate the mechanism above with the computation of A*x+B*y for which the two multiplications can be parallelized. The corresponding graph has","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"three nodes A,x,B,y that corresponds to variables; and\nthree computation nodes, two for each of the multiplications and the other for the addition.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"We start by pre-allocating memory for all nodes","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"using Random\nbegin #hide\nA = rand(Float64,4000,3000)\nx = rand(Float64,3000,1000) \nB = rand(Float64,4000,2500)\ny = rand(Float64,2500,1000) \nAx = Matrix{Float64}(undef,4000,1000)\nBy = similar(Ax)\nAxBy = similar(Ax)\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"and defining the computations for each node","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"using LinearAlgebra\nbegin # hide\nfunction node_Ax!(out::Matrix{F},in1::Matrix{F}, in2::Matrix{F}) where {F} \n    mul!(out,in1,in2)\nend\nfunction node_By!(out::Matrix{F},in1::Matrix{F}, in2::Matrix{F}) where {F} \n    mul!(out,in1,in2)\nend\nfunction node_AxBy!(out::Matrix{F},in1::Matrix{F}, in2::Matrix{F}) where {F} \n    @. out = in1 + in2\nend\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"We used fairly big matrices for which the computation takes some time, as we can see below:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\n@show Threads.nthreads()\nBLAS.set_num_threads(1)\n@show BLAS.get_num_threads()\n@show Base.JLOptions().opt_level\n\nnode_Ax!(Ax,A,x) # compile # hide\nnode_By!(By,B,y) # compile # hide\nnode_AxBy!(AxBy,Ax,By) # compile # hide\n@time begin\n    node_Ax!(Ax,A,x)\n    node_By!(By,B,y)\n    node_AxBy!(AxBy,Ax,By)\nend\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"To implement the parallelization mechanism described above we need 2 event-triggered objects per node:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nvalid=Tuple(Threads.Event(false) for _ in 1:3)\nrequest=Tuple(Threads.Event(true) for _ in 1:3)\nreset.(valid)\nreset.(request)\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"The computation tasks can then be launched using","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\ntasks=[\n    Threads. @spawn while true\n        wait(request[1])\n        if !valid[1].set\n            node_Ax!(Ax,A,x)    # this node's computation\n            notify(valid[1])\n        end\n    end\n\n    Threads. @spawn while true\n        wait(request[2])\n        if !valid[2].set\n            node_By!(By,B,y)    # this node's computation\n            notify(valid[2])\n        end\n    end\n\n    Threads.@spawn while true\n        wait(request[3])\n        if !valid[3].set\n            valid[1].set || notify(request[1])\n            valid[2].set || notify(request[2])\n            valid[1].set || wait(valid[1])\n            valid[2].set || wait(valid[2])\n            node_AxBy!(AxBy,Ax,By)  # this node's computation\n            notify(valid[3])\n        end\n    end\n]\nprintln(sprint(show,\"text/plain\",tasks)) # hide\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"note: Note\nVery similar code is used in computeSpawn! to parallelize the computation of general graphs.","category":"page"},{"location":"lib_representation.html#Parallelism-verification","page":"Representation of computation graphs","title":"Parallelism verification","text":"","category":"section"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"To verify the operation of the approaches outlined above, we make a request for the value of the final node A*x+B*y and wait on the node being valid:","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"begin # hide\nusing ThreadPinning\npinthreads(:cores)\n@show Threads.nthreads()\nBLAS.set_num_threads(1)\n@show BLAS.get_num_threads()\n@show Base.JLOptions().opt_level\n\nfill!(Ax,0.0)\nfill!(By,0.0)\nfill!(AxBy,0.0)\nbegin # compile # hide\n    notify(request[3]) # compile # hide\n    wait(valid[3]) # compile # hide\nend # compile # hide\nreset.(valid)\nprintln(\"valid before :\",getproperty.(valid,:set))\n@time begin\n    notify(request[3])\n    wait(valid[3])\nend\nprintln(\"valid after  :\",getproperty.(valid,:set))\n@assert Ax==A*x \n@assert By==B*y\n@assert AxBy==A*x+B*y\nnothing # hide\nend # hide","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"When multiple hardware threads are available, the time reported by @time is roughly about half, showing a good use of the threads.","category":"page"},{"location":"lib_representation.html","page":"Representation of computation graphs","title":"Representation of computation graphs","text":"note: Note\nWe can see whether the julia threads were successfully \"pinned\" to physical hardware threads using ThreadPinning.threadinfo(), where red means that multiple julia threads are running on the same hardware thread and purple means that the julia thread is really running on a hyperthread. In either case, we should not expect true parallelism. This is often the case when code is run through a GitHub action (as in generating this manual page) on a computer with a single core with Simultaneous Multithreading (SMT).begin # hide\nusing ThreadPinning\n@show Threads.nthreads()\n@show pinthreads(:cores)\nthreadinfo()\nnothing # hide\nend # hide","category":"page"},{"location":"man_guide.html#Basics","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"ComputationGraphs is about improving the speed (and energy consumption) of numerical computations that need to be performed repeatedly, e.g.,","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"one iteration of a numerical optimization algorithm, \none iteration of a filtering/smoothing algorithm,\nrepeated calls to a classification algorithm on different samples, etc.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The computation to be performed is encoded into a data structure ComputationGraph that permits several forms of \"run-time\" optimization, including:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"allocation-free operation\n(partial) re-use of perviously performed computations\nsymbolic differentiation\nsymbolic algebraic simplifications ","category":"page"},{"location":"man_guide.html#What-is-a-*computation-graph*?","page":"Basics","title":"What is a computation graph?","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"A computation graph represents a set of mathematical operations,   performed over a set of variables. Formally, it is represented as a graph with:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"nodes that correspond to variables that hold the result of a mathematical operation, and \nedges that encode the operands used by each mathematical operation.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The edges are directed from the operand (which we call the \"parent node\") to the result of the operation (which we call the \"child node\")","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"For example, the formula","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"    e =  A x -b ^2","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"can be represented by the following tree","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"norm2(A*x-b) (operation is squared-norm \\| \\|^2)\n\nA*x-b        (operation is subtraction -)\n\n A*x      (operation is multiplication *)\n    A\n    x\n b","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"We can recognize two types of nodes:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"A, x, and x are varible nodes that are associated with \"inputs\" to the computation, and\nA*x, A*x-b, and norm2(A*x-b) are computation nodes that are associated with some algebraic operation.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"We will often use the expression \"evaluate a (computation) node\" to mean \"perform the operation associated with a node\". For example, by \"evaluate the node A*x-b\" we mean:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"first fetch the value of the parent node A*x,\nthen fetch the value of the other parent node `b,\nand finally subtract these two values to obtain the value of the child node A*x-b.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"Since step 1) involves a computation node, this step presumes that the parent node A*x has been previously evaluated and the value of this evaluation has been saved; otherwise this node would need to be evaluated, prior to step 1).","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"Computation graphs are represented by the structures ComputationGraph, which encode the graph itself as well as additional information about the variables (size, types, whether the node has been evaluated, its stored value, etc).","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"note: Note\nWhile missing from this example, a third type of node is possible: constant nodes are also associated with \"inputs\" to the computation (like variable nodes), but they never change. Declaring \"input\" nodes as constants typically enables computational savings.","category":"page"},{"location":"man_guide.html#Building-a-computation-graph","page":"Basics","title":"Building a computation graph","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The computation graph above can be created using the following code","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"using ComputationGraphs\ngraph = ComputationGraph{Float64}()\nA = variable(graph, 4, 3)\nx = variable(graph, 3)\nb = variable(graph, 4)\ne = @add graph norm2(A*x-b)\nnothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"Upon execution graph represents a 6-node computation graph:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The 1st assignment creates an empty graph\nThe 2nd assignment creates a variable that stores a 4x3 matrix\nThe 3rd and 4th assignments create 2 variables that store vectors with sizes 3 and 4, respectively.\nThe 5th assignment adds nodes to the graph that store the values of A*x, A*X-b, and norm2(A*X-b).","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"This code only defines a computation graph, but it actually does not performed any computation.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"A computation graph may include several computations that share common variables. For example, we could add to the same graph the computation of the gradient of e with respect to x, which turns out to be","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"nabla_x e = 2A(Ax-b)","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"This can be added to the existing computation graph using","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"grad = @add graph constant(2.0) * adjointTimes(A, A*x - b )\nnothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"This command will recognize that the existing graph already nodes for A*x and A*x-b so only 3 more nodes need to be added: the constant value 2 and the products A'*(A*x-b), 2*A'*(A*x-b); resulting in a graph with 9 nodes.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The reuse of nodes has important implications in terms of reusing computation. Specifically, ","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"Once we compute the gradient nabla_x e, the term A*x-b becomes available and computing e only requires computing norm2 of A*x-b, which is a relatively \"cheap\" computation.\nAlternatively, if we first compute e, then  A*x-b becomes available and computing the gradient only requires multiplying it by 2*A.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"In either case, we can share intermediate results between the computations of e and nabla_x e. ","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"note: Note\nWe used adjointTimes(M,N) to represent the operation M'*N, which really consists of two operations: taking the adjoint/transpose of the first matrix and then multiplying it by the second matrix.In practice, it is generally more efficient (in terms of time and memory) to combine the two operations into a single one, which is generally automatically done by LinearAlgebra. We currently force the user to explicitly decide whether or not to combine the two operations by usingadjointTimes(M,n)  combine, or\nadjoint(M)*N  do not combine.The later option can be better if adjoint(M) will turn out to be useful for other computations.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"warning: Warning\nCurrently, ComputationGraphs only supports a relatively small set of algebraic operations. These are expected to grow and the package matures.","category":"page"},{"location":"man_guide.html#Using-a-computation-graph","page":"Basics","title":"Using a computation graph","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The creation of a computation graph encodes relationships between variables, but does not actually perform any computation. To perform computations we need to:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"first set the values of all variables that appear in the graph, and\nsecond carryout the computations (in the appropriate order)","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The first step uses the set! command to set the values of variable:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"set!(graph, A, [1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0; 10.0 11.0 12.0])\nset!(graph, x, [1.0, 1.0, 1.0])\nset!(graph, b, [2.0, 2.0, 2.0, 2.0])\nnothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"We are now ready to perform the graph computations using compute!, which can take two forms: To recompute the whole graph, we can use","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"compute!(graph)\nnothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"but if we just want to recompute the portion of the graph needed to evaluate the node e, we can use","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"compute!(graph, e)\nnothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"Finally, we can get the value of e using get","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"value = get(graph,e)\nprintln(value)\nnothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"note: Note\nThe values of all nodes in a computation graph are stored as N-dimensional arrays. Vectors are 1-dimensional arrays, matrices 2-dimensional arrays, but higher dimensional arrays can also be used in ComputationGraphsScalars turn out to also be represented as arrays, but 0-dimensional arrays, which in julia always have a single element. 0-dimensional arrays can be created using fill(value) and are displayed as fill(value):x = fill(1.0)\nprintln(x)\nnothing # hide","category":"page"},{"location":"man_guide.html#Reusing-computations","page":"Basics","title":"Reusing computations","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The structure ComputationGraph encodes all the dependencies between the nodes of a computation graph, which enables minimizing computation by maximizing the re-use of computations that have been previously performed.","category":"page"},{"location":"man_guide.html#Doing-all-the-*necessary*-computations,-but-no-more-than-that","page":"Basics","title":"Doing all the necessary computations, but no more than that","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The goal of the compute!(graph) is to make sure that all nodes hold valid value. This does not mean that the function needs to recompute all nodes, since some nodes may have been previously compute and thus may already hold valid values.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"This means that if we call compute!(graph) twice, the second time will actually not perform any computation. This can be seen in the following example that \"fools\" @benchmark into believing that the operation Ax-b for very large matrices/vectors only takes a few nano seconds.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"using ComputationGraphs, BenchmarkTools\ngraph = ComputationGraph{Float64}()\nA = variable(graph, rand(Float64,4000, 3000))\nx = variable(graph, rand(Float64,3000))\nb = variable(graph, rand(Float64,4000))\ne = @add graph norm2(A*x-b)\nusing BenchmarkTools\nbmk=@benchmark compute!($graph)\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true));nothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"To prevent computation re-use, we can use the keyword force=true to force compute! to actually redo the computations:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"using ComputationGraphs, BenchmarkTools\ngraph = ComputationGraph{Float64}()\nA = variable(graph, rand(Float64,4000, 3000))\nx = variable(graph, rand(Float64,3000))\nb = variable(graph, rand(Float64,4000))\ne = @add graph norm2(A*x-b)\nusing BenchmarkTools\nbmk=@benchmark compute!($graph,force=true)\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true));nothing  # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"We now see that the computation actually takes a few milliseconds.","category":"page"},{"location":"man_guide.html#Partial-graph-computations","page":"Basics","title":"Partial-graph computations","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"As noted above, the function compute! can be made node-specific; meaning that it only recomputes the set of nodes that are \"need\" to get the value of a specific node. Also in this case, \"parent nodes\" that already hold valid values do not need to be recomputed.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The function get also checks if the desired node has a valid value and recomputes the node if it does not. Specifically, get(graph,node) implicitly calls compute(graph,node) before returning the value of the node. This means that in the following examples the 2nd function will actually never do any computation:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"compute!(graph)\ncompute!(graph, e)   # no computation","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"compute!(graph, e)\nvalue=get(graph, e)  # no computation, just returns value","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"value=get(graph, e)\ncompute!(graph, e)   # no computation","category":"page"},{"location":"man_guide.html#Redoing-computations-when-*necessary*","page":"Basics","title":"Redoing computations when necessary","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"Initialization of updates in variables are prompted by calling either of the following two functions:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"set!(graph, variable, value) sets the value of the node variable (which must have been created using the command variable) to the value of the array value.\ncoptyto!(graph, node1, node2) copies the value of the node node2 to the node variable (which must have been created using the command variable).","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"When the values of input variables are changed, some (but not necessarily all) nodes may need to be recomputed. ","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"For example, if we use set! or copyto! to change the values of the three variables A, x, b, their children nodes A*x, A*x-b, and norm2(A*x-b) need to be recomputed.\nHowever, if we only use set! or copyto! to change the value of b, the node A*x does not need to be recomputed.","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The functions set! and copyto! are \"smart\" in the sense that they keeps track of these dependencies and only marks as \"invalid\" the \"children\" of the variable that has been changes. The following examples illustrate this: In the first case, the value of x is set so recomputing e requires recomputing the \"expensive\" product A*x:","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"using ComputationGraphs, BenchmarkTools\ngraph = ComputationGraph{Float64}()\nA = variable(graph, rand(Float64,4000, 3000))\nx = variable(graph, rand(Float64,3000))\nb = variable(graph, rand(Float64,4000))\ne = @add graph norm2(A*x-b)\nx0=rand(Float64,size(x))\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 5 # hide\nbmk=@benchmark begin\n        set!($graph, $x, $x0)   # x changes so A*x and A*x-b need to be recomputed\n        value=get($graph, $e)\n    end\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true)); nothing # hide","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"In this example, only the value of b is set so recomputing e can reuse the previous value of A*x. Subtractring the new b and taking the norm are much \"cheaper\" operations ans the compute time decreases from milliseconds to microseconds: ","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"b0=rand(Float64,size(b))\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 5 # hide\nbmk=@benchmark begin\n        set!($graph, $b, $b0)   # b changes so A*x does not need to be recomputed\n        value=get($graph, $e)\n    end\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true));nothing # hide","category":"page"},{"location":"man_guide.html#Allocation-free-computations","page":"Basics","title":"Allocation-free computations","text":"","category":"section"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"The sizes of the arrays associated with all nodes become known as the graph is built, which means that memory can be pre-allocated to store the values associated with every node. ","category":"page"},{"location":"man_guide.html","page":"Basics","title":"Basics","text":"In practice, this means that calls to set!, copyto!, get, and compute! are typically allocation free, as reported above by @benchmark. This greatly contributes to minimizing garbage collection and keeping the compute times small and fairly predictable.","category":"page"},{"location":"man_recipes.html#Recipes","page":"Recipes","title":"Recipes","text":"","category":"section"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"ComputationGraphs includes a few recipes to add to a computation graph the nodes needed to perform a \"common\" computation or to implement a \"common\" algorithm. These recipes are functions of the general form","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"output_nodes = recipe!(graph, input_nodes...; keyword_parameters)","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"where","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"graph is a computation graph that is updated \"in-place\" by adding to it all the nodes needed for the recipe,\ninput_nodes is typically a (named) tuple of nodes from the existing computational graph to which the recipe will be applied,\nkeyword_parameters are specify options for the recipe, and\noutput_nodes is typically a (named) tuple of nodes generated by the recipe. The recipe may add many more \"intermediate\" nodes to graph, but only those in output_nodes contain the recipe's \"final outputs\".","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"The functions D and hessian discussed in [Differentiation] are, in fact, recipes for which the input nodes specify the expression and variable to differentiate and a single output node that corresponds to the derivative.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"Building upon these recipes, ComputationGraphs provides more complex recipes, e.g., to implement common optimization algorithms and to perform inference and training with neural networks.","category":"page"},{"location":"man_recipes.html#Optimization-recipes","page":"Recipes","title":"Optimization recipes","text":"","category":"section"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"gradDescent! is a recipe for performing the computations needed by the classical gradient descent algorithm to minimize a (scalar-valued) loss function J(theta) by adjusting a set of optimization parameters theta.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"    theta^+ = theta - eta nabla_theta J(theta)","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"The recipe used to implement gradient descent is called as follows:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"(; next_theta, eta, gradients) = gradDescent!(graph; loss, theta)","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"where the input parameters include","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"loss is a (scalar-valued) computation node that corresponds to the loss function J(theta),\ntheta is a (named) tuple of variable nodes that correspond to the optimization parameters theta,","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"and the returned values include ","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"eta is a variable node that can be used to set the learning rate eta, \nnext_theta is a (named) tuple of computation nodes that holds the value theta^+ of the optimization parameters after one gradient descent iteration,\ngradients is a (named) tuple of the computation nodes that hold the value of the gradients of the loss function with respect to the different variables in theta.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"To illustrate the use of this recipe, suppose that we want to minimize","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"    J(x) =  A x -b ^2","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"with respect to x. The construct of the computation graph that accomplishes this using gradient descent, we would use:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"using ComputationGraphs\ngraph = ComputationGraph{Float64}()\nA = variable(graph, 4, 3)\nx = variable(graph, 3)\nb = variable(graph, 4)\nloss = @add graph norm2(A*x-b)\ntheta = (;x,)\n(; next_theta, eta, gradients) = gradDescent!(graph; loss, theta)\nnothing # hide","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"With this graph in place, the actual optimization can be carried out as follows:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"using BenchmarkTools\n# Set fixed parameters\nset!(graph, A, [1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0; 10.0 11.0 12.0])\nset!(graph, b, [2.0, 2.0, 2.0, 2.0])\n# Set learning rate\nset!(graph, eta, 0.001)\n# Initialize optimization parameter\nset!(graph, x, [1.0, 1.0, 1.0])\nprintln(\"initial loss: \", get(graph,loss))\n# Gradient descent loop\nnIterations=100\nfor i in 1:nIterations\n    compute!(graph, next_theta)       # compute next value of theta\n    copyto!(graph, theta, next_theta) # execute update\nend\nprintln(\"final loss: \", get(graph,loss))\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 5 # hide\nbmk = @benchmark for i in 1:$nIterations\n    compute!($graph, $next_theta)             # compute next value of theta\n    copyto!($graph, $theta, $next_theta)      # execute update\nend setup=(set!(graph, x, [1.0, 1.0, 1.0]))\nprintln(\"final loss: \", get(graph,loss))\nprintln(sprint(show,\"text/plain\",bmk;context=:color=>true)) # hide\n@assert bmk.allocs==0; nothing                              # hide","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"The recipe adam! for the more complex Adam's algorithm can be found in Examples","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"warning: Warning\nUnlike get, the function copyto! does not implicitly carry out any computations to make sure that the value being copied is valid. Because of this, we need to explicitly call compute! before copyto!.","category":"page"},{"location":"man_recipes.html#Neural-network-recipes","page":"Recipes","title":"Neural-network recipes","text":"","category":"section"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"denseChain! is a recipe for performing inference and training of a dense forward neural network of form:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"    x[1]   = input\n    x[2]   = activation(W[1] * x[1] + b[1])\n    ...\n    x[N-1] = activation(W[N-2] * x[N-2] + b[N-2])\n    output = W[N-1] * x[N-1] + b[N-1]              # no activation in the last layer","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"with training based on loss function of","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"  loss = some_loss_function(output-reference)","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"The recipe used for this is called as follows:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"(; inference, training, theta) = denseChain!(graph; \n        nNodes, inferenceBatchSize, trainingBatchSize, activation,loss)","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"where the input parameters include","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"nNodes::Vector{Int}: Vector with the number of nodes in each layer, starting from the       input and ending at the output layer.\ninferenceBatchSize::Int=1: Number of inputs for each inference batch.       When inferenceBatchSize=0 no nodes will be created for inference.\ntrainingBatchSize::Int=0: Number of inputs for each training batch.        When trainingBatchSize=0 no nodes will be created for training.\nactivation::Function=ComputationGraphs.relu: Activation function.        Use the identity function if no activation is desired.\nloss::Symbol=:mse: Desired type of loss function, among the options:       + :sse = sum of square error       + :mse = mean square error (i.e., sse normalized by the error size)       + :huber = huber function on the error       + :mhuber = huber function on the error, normalized by the error size","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"and the returned tuple includes","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"inference::NamedTuple: named tuple with the inference nodes:       + input NN input for inference       + output NN output for inference       When inferenceBatchSize=0 this tuple is returned empty","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"+ training::NamedTuple: named tuple with the training nodes:         + input NN input for training         + output NN output for training         + reference NN desired output for training         + loss NN loss for training         When trainingBatchSize=0 this tuple is returned empty","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"theta::NamedTuple: named tuple with the NN parameters (all the matrices W and b)","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"To illustrate the use of this recipe, suppose that we want to construct a neural network whose input is an angle in the [0,2*pi] range with two outputs that return the sine and cosine of the angle.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"To accomplish this, will use a network with 1 input, 2 output, a few hidden layers, and relu activation functions.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"We want the computation graph to support:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"inference, i.e., compute the output for a given input;\ntraining, i.e., minimize the loss for a given set of inputs and desired outputs.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"For training we will use a large batch size, but for inference we will only provide one input at a time.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"We start by using denseChain! to construct a graph that performs all the computations needed to do inference and compute the (training) loss function for the network.","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"using ComputationGraphs, Random\ngraph=ComputationGraph{Float32}()\nhiddenLayers=[20,20,20]\n(; inference, training, theta)=denseChain!(graph; \n        nNodes=[1,hiddenLayers...,2], inferenceBatchSize=1, trainingBatchSize=3000,\n        activation=ComputationGraphs.relu, loss=:mse)\nnothing # hide","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"We now generate random (but repeatable) training data:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"Random.seed!(0)\ninput=(2*pi).*rand(Float32,size(training.input))\nreference=vcat(sin.(input),cos.(input))\nset!(graph,training.input,input)\nset!(graph,training.reference,reference)\nnothing # hide","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"Not surprisingly, with a random initialization of the network weights, the loss is large:","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"Random.seed!(0)\nfor k in eachindex(theta)\n    set!(graph,theta[k],randn(Float32,size(theta[k])))\nend\nloss=get(graph,training.loss)\nprintln(\"loss = \",loss)","category":"page"},{"location":"man_recipes.html","page":"Recipes","title":"Recipes","text":"However, we can use an optimization recipe - like gradDescent! or adam! - to train the network. This can be found in the example Neural network training.","category":"page"},{"location":"lib_public.html#API","page":"API","title":"API","text":"","category":"section"},{"location":"lib_public.html#Contents","page":"API","title":"Contents","text":"","category":"section"},{"location":"lib_public.html","page":"API","title":"API","text":"Pages = [\"lib_public.md\"]\nDepth = 2:3","category":"page"},{"location":"lib_public.html#Public-Interface","page":"API","title":"Public Interface","text":"","category":"section"},{"location":"lib_public.html#Graph-creation","page":"API","title":"Graph creation","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.ComputationGraph","page":"API","title":"ComputationGraphs.ComputationGraph","text":"Structure use to store the computation graph.\n\nFields:\n\nnodes::Vector{AbstractNode}: vector with all nodes in the graph in topological order: children       always appear after parents.\nchildren::Vector{Vector{Int}}: vector with all the children of each node\nparents::Vector{Vector{Int}}: vector with all the parents of each node\nvalidValue::BitVector: boolean vector, true indicates that node contains a valid value\n`computewithancestors::Vector{FunctionWrapper}: vector of function to compute each node all the required ancestors\ncount::Vector{UInt}: number of times each node has been computed since last resetLog!\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.@add","page":"API","title":"ComputationGraphs.@add","text":"@add graph expression\n\nMacro to add a complex expression into a computation graph. \n\nThis macro \"breaks\" the complex expression to elementary subexpressions and add them all to the graph.\n\nParameters:\n\ngraph::ComputationGraph{TypeValue}: graph where expression will be stored\nexpression::Expr: expression to be added to the graph\n\nReturns:\n\nNode::AbstractNode: graph node for the final expression \n\nExample:\n\nThe following code provides two alternatives to create a computation graph to evaluate     err = ||A *x -b ||^2\n\nwithout the @add macro  julia  using ComputationGraphs  gr=ComputationGraph{Float32}()  A = variable(gr,3,4)  x = variable(gr,4)  b = variable(gr,3)  Ax  = *(gr,A,x)  Axb = -(gr,Ax,b)  err = norm2(gr,Axb)  display(gr)\nwithout the @add macro  julia  using ComputationGraphs  gr=ComputationGraph{Float32}()  A = @add gr variable(3,4)  x = @add gr variable(4)  b = @add gr variable(3)  err = @add gr norm2(times(A,x)-b)  display(gr)\n\n\n\n\n\n","category":"macro"},{"location":"lib_public.html#ComputationGraphs.variable","page":"API","title":"ComputationGraphs.variable","text":"variable(graph,dims)\nvariable(graph,dims...)\nvariable(graph, value)\n\nCreates a variable of the given dimension\n\nParameters:\n\ngraph::ComputationGraph{TypeValue}: Computation graph where variable will be stored.\ndims::NTuple{N,Int}: Desired dimension of the variable. An empty tuple () results in a scalar variable.\nvalue::AbstractArray: Initial value for the variable, which implicitly defines its dimension.       If value is a scalar, it is first converted to a 0-dimensional array using fill(value).\n\nReturns:\n\nnode of the computation graph associated with the variable created\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.constant","page":"API","title":"ComputationGraphs.constant","text":"constant(graph, value)\n\nCreates a (constant) array equal to the given value. \n\nParameters:\n\ngraph::ComputationGraph{TypeValue}: Computation graph where the array will be stored.\nvalue::AbstractArray: Desired value for the array.        If value is a scalar, it is first converted to a 0-dimensional array using fill(value).\n\nReturns:\n\nnode of the computation graph associated with an array created\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.zeros","page":"API","title":"Base.zeros","text":"zeros(graph, dims)\nzeros(graph, dims...)\n\nCreates an array filled with 0's\n\nParameters:\n\ngraph::ComputationGraph{TypeValue}: computation graph where the array will be stored\ndims::NTuple{N,Int}: dimension of the array\n\nReturns:\n\nnode of the computation graph associated with an array filled with zero(TypeValue)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.ones","page":"API","title":"Base.ones","text":"ones(graph, dims)\nones(graph, dims...)\n\nCreates an array filled with 1's\n\nParameters:\n\ngraph::ComputationGraph{TypeValue}: computation graph where array will be stored\ndims::NTuple{N,Int}: dimension of the array\n\nReturns:\n\nnode of the computation graph associated with an array filled with one(TypeValue)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.unitvector","page":"API","title":"ComputationGraphs.unitvector","text":"unitvector(dims,k)\n\nCreates the k-th vector of the canonical basis for the linear space with dimension dims.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.size","page":"API","title":"Base.size","text":"size(node)\nsize(graph, node)\n\nReturns a tuple with the size of the array associated with a node of a computation graph.\n\n\n\n\n\nsize(node, dim)\nsize(graph, node, dim)\n\nReturns the size of the array associated with a node of a computation graph, along dimension dim.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.length","page":"API","title":"Base.length","text":"length(node)\nlength(graph, node)\n\nReturns the number of entries of the array associated with a node of a computation graph.\n\n\n\n\n\nNumber of nodes in the graph.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.typeofvalue","page":"API","title":"ComputationGraphs.typeofvalue","text":"Type of the node's value.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.similar","page":"API","title":"Base.similar","text":"similar(node)\nsimilar(graph, node)\n\nCreates an uninitialized array with the same type and size as the graph node.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.eltype","page":"API","title":"Base.eltype","text":"eltype(node)\n\nReturns the type of the entries of a node.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.memory","page":"API","title":"ComputationGraphs.memory","text":"Total memory for all the variables stored in the graph.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.nodeValue","page":"API","title":"ComputationGraphs.nodeValue","text":"nodeValue(node)\n\nReturns the current value of a node (without any evaluation)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.Multimedia.display","page":"API","title":"Base.Multimedia.display","text":"display(node)\ndisplay(nodes)\n\nDisplay one node of a computation graph or a tuple of nodes\n\n\n\n\n\ndisplay(graph;topTimes=false)\n\nDisplay the nodes of a computation graph.\n\nWhen topTimes=true only displays the nodes with the largest total computation times (and hides information about parents/children).\n\n\n\n\n\ndisplay(graph,node;withParents=true)\n\nWhen withParents=true shows the full expression needed compute a specific node, otherwise only shows the specific node (as in display(node)).\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Operations-supported","page":"API","title":"Operations supported","text":"","category":"section"},{"location":"lib_public.html#Base.adjoint","page":"API","title":"Base.adjoint","text":"adjoint() computes adjoint/transpose of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.adjoint_","page":"API","title":"ComputationGraphs.adjoint_","text":"adjoint() computes adjoint/transpose of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.adjointTimes","page":"API","title":"ComputationGraphs.adjointTimes","text":"adjointTimes(A,x)= A'*x computes the product of the adjoint of the matrix A with a matrix/vector x\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.adjointTimesExpandColumns","page":"API","title":"ComputationGraphs.adjointTimesExpandColumns","text":"adjointTimesExpandColumns(A,x,rows) = A'*expandColumns(x,rows) computes the product of the adjoint of the matrix A with expandColumns(x,rows)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.affine","page":"API","title":"ComputationGraphs.affine","text":"affine(A,x,b) = A*x .+ b where b is a vector, x can be a vector or a matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.affineRows","page":"API","title":"ComputationGraphs.affineRows","text":"affineRows(A,x,b,rows) = (A*x+b)[rows,:]\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.column","page":"API","title":"ComputationGraphs.column","text":"column(A,k) returns the column k of A as a vector\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.divideScalar","page":"API","title":"ComputationGraphs.divideScalar","text":"divideScalar(a, b) = a ./ b, where b is a scalar\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.expandColumns","page":"API","title":"ComputationGraphs.expandColumns","text":"expandColumns(a,rows,nRows)\n\nExpands a vector a into a matrix A as follows:      Given an n-vector a , returns an nRows x n matrix A with          A[i,j] = a[j] if i==rows[j] else 0\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.expandColumnsTimesAdjoint","page":"API","title":"ComputationGraphs.expandColumnsTimesAdjoint","text":"expandColumnsTimesAdjoint(x,y,rows,nRows)=expandColumns(x,rows,nRows)*y'\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.exponentScalar","page":"API","title":"ComputationGraphs.exponentScalar","text":"exponentScalar(a, b) = a .^ b, where b is a scalar\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.findMaxRow","page":"API","title":"ComputationGraphs.findMaxRow","text":"y=findMaxRow(A)\n\nCreates an integer-valued vector y with as many entries as columns of A, where y[j] is equal to the index of the row of the largest entry in columns j of A.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.huber","page":"API","title":"ComputationGraphs.huber","text":"huber() computes the huber loss of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.maxRow","page":"API","title":"ComputationGraphs.maxRow","text":"maxRow(A) computes a vector y with as many entries as columns of A, where y[j] is equal to the largest entry in columns j of A\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.minus","page":"API","title":"ComputationGraphs.minus","text":"minus() unitary minus of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.norm1","page":"API","title":"ComputationGraphs.norm1","text":"norm1() computes the sum of the absolute values of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.norm2","page":"API","title":"ComputationGraphs.norm2","text":"norm2() computes the sum of the square values of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.pointDivide","page":"API","title":"ComputationGraphs.pointDivide","text":"pointDivide(a, b) = a ./ b\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.pointTimes","page":"API","title":"ComputationGraphs.pointTimes","text":"pointTimes(a, b) = a .* b\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.scalarDivide","page":"API","title":"ComputationGraphs.scalarDivide","text":"scalarDivide(a, b) = a ./ b, where a is a scalar\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.scalarTimes","page":"API","title":"ComputationGraphs.scalarTimes","text":"scalarTimes(a,M)= a .* M computes the product of a scalar a by a matrix M\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.selectRows","page":"API","title":"ComputationGraphs.selectRows","text":"selectRows(A,rows) = y, where y[j] =A[rows[j],j]\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.subtract","page":"API","title":"ComputationGraphs.subtract","text":"a - b subtraction operator\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.sumColumns","page":"API","title":"ComputationGraphs.sumColumns","text":"sumColumns(A) returns a vector with the sums of the columns of a matrix A\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.sumExpandColumns","page":"API","title":"ComputationGraphs.sumExpandColumns","text":"sumColumns(ExpandColumns(x,rows,nRows))\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.times","page":"API","title":"ComputationGraphs.times","text":"times(A,x) computes the product of a matrix A by a matrix/vector x\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.timesAdjoint","page":"API","title":"ComputationGraphs.timesAdjoint","text":"timesAdjoint(x, y') = x * y'\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.timesAdjointOnes","page":"API","title":"ComputationGraphs.timesAdjointOnes","text":"timesAdjointOnes(x,n)=x*ones(n)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.unitTimesAdjoint","page":"API","title":"ComputationGraphs.unitTimesAdjoint","text":"unitTimesAdjoint(y,dims,k) = unitvector(dims,k)*y'\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.:+","page":"API","title":"Base.:+","text":"a + b addition operator\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.:-","page":"API","title":"Base.:-","text":"-() unitary minus operator for a vector or matrix\n\n\n\n\n\na - b subtraction operator\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.:*","page":"API","title":"Base.:*","text":"a * b maps to times() or scalarTimes() depending on the sizes of the arguments\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.:^","page":"API","title":"Base.:^","text":"a ^ b maps to exponentScalar(a,b)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#LogExpFunctions.logistic","page":"API","title":"LogExpFunctions.logistic","text":"logistics(x)=1/(1+exp(-x)) computes the logistics function of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.relu","page":"API","title":"ComputationGraphs.relu","text":"relu() computes the relu (max with 0) of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.ddlogistic","page":"API","title":"ComputationGraphs.ddlogistic","text":"ddlogistic(x)=(exp(-2x)-exp(-x)) /(1+exp(-x))^3\n\ncomputes the 2nd-derivative of the logistics function of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.dlogistic","page":"API","title":"ComputationGraphs.dlogistic","text":"dlogistics(x)=exp(-x)/(1+exp(-x))^2\n\ncomputes the derivative of the logistics function of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.exp","page":"API","title":"Base.exp","text":"exp() computes the exponential of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.heaviside","page":"API","title":"ComputationGraphs.heaviside","text":"heaviside() computes the heaviside (>0 indicator) of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.sat","page":"API","title":"ComputationGraphs.sat","text":"sat() computes the saturation function of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.sign","page":"API","title":"Base.sign","text":"sign() computes the sign function of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.sqrt","page":"API","title":"Base.sqrt","text":"sqrt() takes the square root of all entries of a vector or matrix\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Differentiation","page":"API","title":"Differentiation","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.D","page":"API","title":"ComputationGraphs.D","text":"Y = D(graph, F, P)\nY = D(graph, V, F, P)\n\nComputes the partial derivative of the expression encoded in the node F with respect to the variable encoded in the node P, along the direction V. Formally, Y is a scalar/vector/matrix with the same size as the variable F, with its jth entry equal to\n\nYj = sum_i Vi nabla_Pj Fi\n\nwhere nabla_Xj Fi the partial derivative of the ith entry of F with respect to the jth entry of P.\n\nThe direction V can be omitted when F is a scalar, in which case \n\nYj = nabla_Pj F\n\nParameters\n\ngraph::ComputationGraph: Computation graph encoding the relevant expressions and variables.\nV::Node: Direction with respect the partial derivative is computed.        This node needs to have the same size as F.\nF::Node: Expression to be differentiated.\nP::NodeVariable: Variable with respect to F will be differentiated.        This node must have been created using variable\n\nReturns\n\nY::Node: Node that encodes the expression of the partial derivative (added to the graph if it       was not already part of it.)        This node will have the same size as P.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.hessian","page":"API","title":"ComputationGraphs.hessian","text":"Y = hessian(graph, F, P, Q)\n\nComputes the Hessian matrix of the expression encoded in the (scalar-valued) node F with respect to the variables encoded in the (vector-values) nodes P and Q. Formally, Y is a matrix with its (i,j)th entry equal to\n\nYij = nabla_Pi nabla_Qj F\n\nwhere nabla_X denotes partial derivative with respect to X.\n\nParameters\n\ngraph::ComputationGraph: Computation graph encoding the relevant expressions and variables.\nF::Node: Expression to be differentiated.\nP::NodeVariable: First variable with respect to F will be differentiated.        This node must have been created using variable\nQ::NodeVariable: Second variable with respect to F will be differentiated.        This node must have been created using variable\n\nReturns\n\nY::Node: Node that encodes the expression of the Hessian matrix (added to the graph if it       was not already part of it.) \n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Graph-computations","page":"API","title":"Graph computations","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.set!","page":"API","title":"ComputationGraphs.set!","text":"set!(graph,node,value)\nset!(graph,nodes,values)\n\nUpdate a variable node\n\nset value of a variable node \nmark all the children as having invalid values\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.compute!","page":"API","title":"ComputationGraphs.compute!","text":"Recompute the whole graph\n\n\n\n\n\nRecompute only what is needed to get a node\n\n\n\n\n\nRecompute only what is needed to get a vector/tuple of node\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.get","page":"API","title":"Base.get","text":"Get the value of a node, performing whatever computations are needed.\n\n\n\n\n\nGet the values of a list of node.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.copyto!","page":"API","title":"Base.copyto!","text":"performing whatever computations are need for source node to be valid\ncopy value of source to destination node\nmark all children of the destination node as having invalid values\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Recipes","page":"API","title":"Recipes","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.gradDescent!","page":"API","title":"ComputationGraphs.gradDescent!","text":"(;next_theta,eta,gradients) = gradDesc(graph; loss, theta)\n\nRecipe used to performs the computations needed by the classical gradient descent algorithm to minimize a (scalar-valued) loss function J(\theta) by adjusting a set of optimization parameters theta, according to\n\n    theta^+ = theta - eta nabla_theta J(theta)\n\nParameters:\n\ngraph::ComputationGraph{TypeValue}; Computation graph that is updated \"in-place\" by adding to it       all the nodes needed to perform one step of gradient descent.\nloss::Node: Scalar-valued computation node that corresponds to the loss function J(theta)\ntheta::NamedTuple`: Named tuple with the variable nodes that correspond to the optimization       parameters theta.\n\nReturns: named tuple with\n\neta::Node: Scalar-valued variable node that can be used to set the learning rate eta.\nnext_theta::Tuple: Named tuple with the computation nodes that holds the value theta^+ of       the optimization parameters after one gradient descent iteration\ngradients::NamedTuple: Named tuple of the computation nodes that hold the value of the gradients of       the loss function with respect to the different variables in theta.\n\nExample:\n\nusing ComputationGraphs\n\ngraph = ComputationGraph{Float64}()\n\n# Define optimization parameters and loss function\nA = variable(graph, 4, 3)\nx = variable(graph, 3)\nb = variable(graph, 4)\nloss = @add graph norm2(times(A, x) - b)\n\n# Call gradDescent! recipe\ntheta = (;x,)\n(; next_theta, eta, gradients) = gradDescent!(graph; loss, theta)\n\n# Set fixed parameters\nset!(graph, A, [1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0; 10.0 11.0 12.0])\nset!(graph, b, [2.0, 2.0, 2.0, 2.0])\n\n# Set learning rate\nset!(graph, eta, 0.001)\n\n# Initialize optimization parameter\nset!(graph, x, [1.0, 1.0, 1.0])\nprintln(\"initial loss: \", get(graph,loss))\n\n# Gradient descent loop\nfor i in 1:100\n    compute!(graph, next_theta)       # compute next value of theta\n    copyto!(graph, theta, next_theta) # execute update\nend\nprintln(\"final loss: \", get(graph,loss))\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.adam!","page":"API","title":"ComputationGraphs.adam!","text":"(;  eta, beta1, beta2, epsilon,\n    init_state, state, next_state,\n    next_theta, gradients) = adam!(graph; loss, theta)\n\nRecipe used to performs the computations needed by the Adam method to minimize a (scalar-valued) loss function J(theta) by adjusting a set of optimization parameters theta.\n\nThe algorithm is described in Adam, using the comment just before section 2.1 for a more efficient implementation.\n\nParameters:\n\ngraph::ComputationGraph{TypeValue}; Computation graph that is updated \"in-place\" by adding to it       all the nodes needed to perform one step of gradient descent.\nloss::Node: Scalar-valued computation node that corresponds to the loss function J(theta)\ntheta::NamedTuple: Named tuple with the variable nodes that correspond to the optimization       parameters theta.\n\nReturns: named tuple the following nodes/tuples of nodes\n\neta: Scalar-valued variable node used to set the learning rate eta.\nbeta1: Scalar-valued variable node used to set Adam's beta1 parameter.\nbeta2: Scalar-valued variable node used to set Adam's beta2 parameter.\nepsilon: Scalar-valued variable node used to set Adam's epsilon parameter.\ninit_state, state, next_state: Adam's internal state initializer, current value, and next  value, which include the iteration number and the 2 moments\nnext_theta::Tuple: value theta^+ of the optimization parameters after one gradient       descent iteration\ngradients: gradients of the loss function with respect to the different variables in theta.\n\nExample:\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.denseChain!","page":"API","title":"ComputationGraphs.denseChain!","text":"(; inference, training, theta) = denseChain!(graph; \n    nNodes, inferenceBatchSize, trainingBatchSize, activation,loss)\n\nRecipe used construct a graph for inference and training of a dense forward neural network.\n\n    x[1]   = input\n    x[2]   = activation(W[1] * x[1] + b[1])\n    ...\n    x[N-1] = activation(W[N-2] * x[N-2] + b[N-2])\n    output = W[N-1] * x[N-1] + b[N-1]              # no activation in the last layer\n\nwith a loss function of\n\n  loss = lossFunction(output-reference)\n\nParameters:\n\ngraph::ComputationGraph{TypeValue}; Computation graph that is updated \"in-place\" by adding to it       all the nodes needed to perform one step of gradient descent.\nnNodes::Vector{Int}: Vector with the number of nodes in each layer, starting from the       input and ending at the output layer.\ninferenceBatchSize::Int=1: Number of inputs for each inference batch.       When inferenceBatchSize=0 no nodes will be created for inference.\ntrainingBatchSize::Int=0: Number of inputs for each training batch.        When trainingBatchSize=0 no nodes will be created for training.\nactivation::Function=ComputationGraphs.relu: Activation function.        Use the identity function if no activation is desired.\nloss::Symbol=:mse: Desired type of loss function, among the options:\n  + :sse = sum of square error\n  + :mse = mean-square error (i.e., sse normalized by the error size)\n  + :huber = huber function on the error\n  + :mhuber = huber function on the error, normalized by the error size\n\nReturns: named tuple with the following fields\n\ninference::NamedTuple: named tuple with the inference nodes:\n  + `input` NN input for inference\n  + `output` NN output for inference\n  When `inferenceBatchSize=0` this tuple is returned empty\n\n+ training::NamedTuple: named tuple with the training nodes:         + input NN input for training         + output NN output for training         + reference NN desired output for training         + loss NN loss for training\n\n    When `trainingBatchSize=0` this tuple is returned empty\n\ntheta::NamedTuple: named tuple with the NN parameters (all the matrices W and b)\n\nExample:\n\nusing ComputationGraphs, Random\ngraph=ComputationGraph{Float32}()\n(; inference, training, theta)=denseChain!(graph; \n        nNodes=[1,20,20,20,2], inferenceBatchSize=1, trainingBatchSize=3,\n        activation=ComputationGraphs.relu, loss=:mse)\n\n# (repeatable) random initialization of the weights\nRandom.seed!(0)\nfor k in eachindex(theta)\n    set!(graph,theta[k],randn(Float32,size(theta[k])))\nend\n\n# Compute output for a random input\ninput=randn(Float32,size(inference.input))\nset!(graph,inference.input,input)\noutput=get(graph,inference.output)\nprintln(\"input = \",input,\", output = \",output)\n\n# compute loss for a batch of random inputs and desired outputs (reference)\ninput=randn(Float32,size(training.input))\nreference=randn(Float32,size(training.reference))\nset!(graph,training.input,input)\nset!(graph,training.reference,reference)\nloss=get(graph,training.loss)\nprintln(\"inputs = \",input,\", loss = \",loss)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.denseChain","page":"API","title":"ComputationGraphs.denseChain","text":"denseChain(TypeValue;\n    nNodes=[],\n    W=TypeArray{TypeValue,2}[],\n    b=TypeArray{TypeValue,1}[],\n    trainingBatchSize,\n    inferenceBatchSize,\n    activation=ComputationGraphs.relu,\n    loss::Symbol=:sse,\n    optimizer=NoOptimizer(),\n    includeGradients=false,\n    codeName=\"\",\n    parallel=false\n)\n\nCreate computation graph for a dense forward neural network, defined as follows:\n\n```\nx[1]   = input\n\nz[k]   = W[k] * x[k] + b[k]    for k in 1,...,K\n\nx[k+1] = activation(z[k])            for k in 1,...,K-1\n\noutput = z[K]\n\nloss = norm2(output-desiredOutput)\n\ng[loss,W[k]] = gradient(loss,W[k]) for k in 1,...,K\n\ng[loss,b[k]] = gradient(loss,b[k]) for k in 1,...,K\n```\n\nParameters:\n\n::Type{TypeValue}: default type for the values of the computation graph nodes\nnNodes::Vector{Int}=Int[]: vector with the number of nodes in each layer, starting from the       input and ending at the output layer.\nW::Vector{TypeArray{TypeValue,2}}=TypeArray{TypeValue,2}[]:\nb::Vector{TypeArray{TypeValue,1}}=TypeArray{TypeValue,1}[]:\ntrainingBatchSize::Int:\ninferenceBatchSize::Int:\nactivation::Function=ComputationGraphs.relu:\nloss::Symbol=:sse:\noptimizer::Op=NoOptimizer():\nincludeGradients::Bool=false:\ncodeName::String=\"\":\nparallel::Bool=false:\n\nReturns: Named tuple with fields\n\ngraph\nioNodes \nparameterNodes \ntrainingNodes\noptimizerNodes\ncode\nnOpsI2O\n\nNumber of forward operations to compute output:\n\nz[k]: \n# prods = sum(size(W[k],2)*(size(W[k],1)) for k in 1:K)\n# sums  = sum(size(W[k],2)*(size(W[k],1)) for k in 1:K)\nx[k+1]:\n# activation = sum(size(W[k],2) for k in 1:K-1)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.denseQlearningChain","page":"API","title":"ComputationGraphs.denseQlearningChain","text":"Create computation graph for a dense forward neural network used to store reinforcement learning's Q-function.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.denseChain_FluxZygote","page":"API","title":"ComputationGraphs.denseChain_FluxZygote","text":"Construct dense forward neural network using Flux+Zygote\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.denseChain_FluxEnzyme","page":"API","title":"ComputationGraphs.denseChain_FluxEnzyme","text":"Construct dense forward neural network using Flux+Enzyme\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Parallelization","page":"API","title":"Parallelization","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.computeSpawn!","page":"API","title":"ComputationGraphs.computeSpawn!","text":"computeSpawn!(graph)\n\nSpans a set of tasks for parallel evaluation of a computation graph.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.syncValid","page":"API","title":"ComputationGraphs.syncValid","text":"syncValid(graph)\n\nUpdates graph.validEvents::Threads.Event with graph.validValues::BitValue.\n\nUsage:\n\nThis function is automatically called from within ComputationGraphs.computeSpawn!(graph).\nIt needs to be explicitly called if ComputationGraphs.set! or ComputationGraphs.copyto! is called upon any variable after ComputationGraphs.computeSpawn!(graph) was issued.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.request","page":"API","title":"ComputationGraphs.request","text":"request(graph, node::Node)\nrequest(graph, node::NTuple{Node})\nrequest(graph, node::NamedTuple{Node})\n\nRequests parallel evaluation of a node or a tuple of nodes. \n\nPresumes a previous call to computeSpawn!(graph)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Base.wait","page":"API","title":"Base.wait","text":"wait(graph, node::Node)\nwait(graph, node::NTuple{Node})\nwait(graph, node::NamedTuple{Node})\n\nWaits for the evaluation of a node or a tuple of nodes, after an appropriate computation request made using request(graph, node(s))\n\nPresumes a previous call to computeSpawn!(graph)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.computeUnspawn!","page":"API","title":"ComputationGraphs.computeUnspawn!","text":"computeUnspawn!(graph)\n\nTerminates the tasks spawned by ComputationGraphs.computeSpawn!(graph)\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Code-generation","page":"API","title":"Code generation","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.Code","page":"API","title":"ComputationGraphs.Code","text":"Structure used to generate dedicated code:\n\nFields\n\nparallel::Bool=false: \n  1) When `true` the `valid` flags are implemented with `Threads.Event`s,\n      otherwise just a `Bool` \n  \n  2) When `true` each node has a `Threads.Task` that computes the node\n     as needed.\nunrolled::Bool=false: \n  1) When `true`, the code generated for `get` uses a single function with nested `if`    \n     statements to compute nodes on demand. \n\n     This can lead to very large functions for big graphs. \n  \n     Parallel computation is not supported in this mode.\n\n  2) When `false`, each node has its own `compute` function that (recursively) calls the    \n     parents' `compute` functions.\ncount::Bool=true: When true, the generated code includes counters for how many times each       node's computation function has been called.\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.sets!","page":"API","title":"ComputationGraphs.sets!","text":"Add set!'s to code\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.computes!","page":"API","title":"ComputationGraphs.computes!","text":"Add computes's to code\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.gets!","page":"API","title":"ComputationGraphs.gets!","text":"Add get!'s to code\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.copies!","page":"API","title":"ComputationGraphs.copies!","text":"Add copyto!'s to code\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Internal-functions","page":"API","title":"Internal functions","text":"","category":"section"},{"location":"lib_public.html#Graph-definition","page":"API","title":"Graph definition","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.@newnode","page":"API","title":"ComputationGraphs.@newnode","text":"@newnode name{C1,...,C2}::outputShape\n@newnode name{Nparameters,C1,...,C2}::outputShape\n\nMacro used to create a new computation node type, where \n\nC1,...,C2 represent the operands\nNparameters (optional) represents the number of parameters, which are fixed (as opposed to the     operands)\noutputShape is the size of the result and \ncan be a constant Tuple, as in\n@newnode norm2{x}::()\ncan use C1,...,C2 (especially their sizes), e.g.,\n@newnode mult!{A,B}::(size(C1,1),size(C2,2))\ncan use the values of the parameters, denoted by par1, par2, ...; as in \n@newnode timesAdjointOnes{1,x}::(size(x, 1), par1)\n\nThis macro then generates\n\n\"\"\" \nNode of a computation graph used to represent the result of name()\n\"\"\"\nstruct NodeName{TP<:Tuple,TPI<:Tuple,TV<:AbstractArray,TC} <: ComputationGraphs.AbstractNode\n    id::Int\n    parameters::TP\n    parentIds::TPI\n    value::TV\n    compute!::TC\nend\n\nexport name\nname(graph::ComputationGraph{TypeValue},C1::T1,C2::T2,par1,par2,\n) where {TypeValue,T1<:AbstractNode,T2<:AbstractNode} =\n    push!(graph,NodeName,cg_name!,(par1,par2),(C1.id,C2.id),(c1.value,C2.value),outputShape)\n\n#= not yet implemented\n@inline function compute!(node::NodeName{Tuple{Int},2,Tuple{TP1V,TP2V},TV}) where {TV<:AbstractArray}\n    #C1 = node.parents[1]\n    #C2 = node.parents[2]\n    #@assert !specialComputation(C1) \"$(typeof(node))($(typeof.(n.parents))) not implemented\"\n    #@assert !specialComputation(C2) \"$(typeof(node))($(typeof.(n.parents))) not implemented\"\n    cg_name!(n.value,node.parentValues...,node.parameters...)\nend\n=#\n\n\n\n\n\n","category":"macro"},{"location":"lib_public.html#Base.push!","page":"API","title":"Base.push!","text":"Add node to graph (avoiding repeated nodes).\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.nodesAndParents","page":"API","title":"ComputationGraphs.nodesAndParents","text":"List with all the parents of a set of node.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.add2children","page":"API","title":"ComputationGraphs.add2children","text":"Add node id to all its parents, parents's parents, etc.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.children","page":"API","title":"ComputationGraphs.children","text":"List with all the children of a set of node.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.AbstractNode","page":"API","title":"ComputationGraphs.AbstractNode","text":"All nodes\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.AbstractConstantNode","page":"API","title":"ComputationGraphs.AbstractConstantNode","text":"Nodes that never change (no sets & zero derivative)\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.AbstractSpecialNode","page":"API","title":"ComputationGraphs.AbstractSpecialNode","text":"Nodes for which \"shortcuts\" in computation are possible\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.noComputation","page":"API","title":"ComputationGraphs.noComputation","text":"Nodes that do not require re-computation after creation\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.NodeAdjoint_","page":"API","title":"ComputationGraphs.NodeAdjoint_","text":"Node of a computation graph used to represent the result of adjoint_()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeAdjointTimes","page":"API","title":"ComputationGraphs.NodeAdjointTimes","text":"Node of a computation graph used to represent the result of adjointTimes()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeAdjointTimesExpandColumns","page":"API","title":"ComputationGraphs.NodeAdjointTimesExpandColumns","text":"Node of a computation graph used to represent the result of adjointTimesExpandColumns()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeAffine","page":"API","title":"ComputationGraphs.NodeAffine","text":"Node of a computation graph used to represent the result of affine()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeAffineRows","page":"API","title":"ComputationGraphs.NodeAffineRows","text":"Node of a computation graph used to represent the result of affineRows()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeColumn","page":"API","title":"ComputationGraphs.NodeColumn","text":"Node of a computation graph used to represent the result of column()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeConstant","page":"API","title":"ComputationGraphs.NodeConstant","text":"Node of a computation graph used to represent a constant whose value cannot be changed. It is created by constant().\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeDdlogistic","page":"API","title":"ComputationGraphs.NodeDdlogistic","text":"Node of a computation graph used to represent the result of ddlogistic()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeDivideScalar","page":"API","title":"ComputationGraphs.NodeDivideScalar","text":"Node of a computation graph used to represent the result of divideScalar()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeDlogistic","page":"API","title":"ComputationGraphs.NodeDlogistic","text":"Node of a computation graph used to represent the result of dlogistic()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeDot_","page":"API","title":"ComputationGraphs.NodeDot_","text":"Node of a computation graph used to represent the result of dot_()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeExp_","page":"API","title":"ComputationGraphs.NodeExp_","text":"Node of a computation graph used to represent the result of exp_()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeExpandColumns","page":"API","title":"ComputationGraphs.NodeExpandColumns","text":"Node of a computation graph used to represent the result of expandColumns()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeExpandColumnsTimesAdjoint","page":"API","title":"ComputationGraphs.NodeExpandColumnsTimesAdjoint","text":"Node of a computation graph used to represent the result of expandColumnsTimesAdjoint()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeExponentScalar","page":"API","title":"ComputationGraphs.NodeExponentScalar","text":"Node of a computation graph used to represent the result of exponentScalar()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeFindMaxRow","page":"API","title":"ComputationGraphs.NodeFindMaxRow","text":"Node of a computation graph used to represent the result of findMaxRow()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeHeaviside","page":"API","title":"ComputationGraphs.NodeHeaviside","text":"Node of a computation graph used to represent the result of heaviside()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeHuber","page":"API","title":"ComputationGraphs.NodeHuber","text":"Node of a computation graph used to represent the result of huber()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeLogistic_","page":"API","title":"ComputationGraphs.NodeLogistic_","text":"Node of a computation graph used to represent the result of logistic_()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeMaxRow","page":"API","title":"ComputationGraphs.NodeMaxRow","text":"Node of a computation graph used to represent the result of maxRow()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeMinus","page":"API","title":"ComputationGraphs.NodeMinus","text":"Node of a computation graph used to represent the result of minus()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeNorm1","page":"API","title":"ComputationGraphs.NodeNorm1","text":"Node of a computation graph used to represent the result of norm1()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeNorm2","page":"API","title":"ComputationGraphs.NodeNorm2","text":"Node of a computation graph used to represent the result of norm2()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeOnes","page":"API","title":"ComputationGraphs.NodeOnes","text":"Node of a computation graph used to represent a constant equal to an array of ones. It is created by ones().\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodePointDivide","page":"API","title":"ComputationGraphs.NodePointDivide","text":"Node of a computation graph used to represent the result of pointDivide()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodePointTimes","page":"API","title":"ComputationGraphs.NodePointTimes","text":"Node of a computation graph used to represent the result of pointTimes()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeRelu","page":"API","title":"ComputationGraphs.NodeRelu","text":"Node of a computation graph used to represent the result of relu()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeSat","page":"API","title":"ComputationGraphs.NodeSat","text":"Node of a computation graph used to represent the result of sat()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeScalarTimes","page":"API","title":"ComputationGraphs.NodeScalarTimes","text":"Node of a computation graph used to represent the result of scalarTimes()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeSelectRows","page":"API","title":"ComputationGraphs.NodeSelectRows","text":"Node of a computation graph used to represent the result of selectRows()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeScalarDivide","page":"API","title":"ComputationGraphs.NodeScalarDivide","text":"Node of a computation graph used to represent the result of scalarDivide()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeSign_","page":"API","title":"ComputationGraphs.NodeSign_","text":"Node of a computation graph used to represent the result of sign_()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeSqrt_","page":"API","title":"ComputationGraphs.NodeSqrt_","text":"Node of a computation graph used to represent the result of sqrt_()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeSubtract","page":"API","title":"ComputationGraphs.NodeSubtract","text":"Node of a computation graph used to represent the result of subtract()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeSumColumns","page":"API","title":"ComputationGraphs.NodeSumColumns","text":"Node of a computation graph used to represent the result of sumColumns()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeSumExpandColumns","page":"API","title":"ComputationGraphs.NodeSumExpandColumns","text":"Node of a computation graph used to represent the result of sumExpandColumns()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeTimes","page":"API","title":"ComputationGraphs.NodeTimes","text":"Node of a computation graph used to represent the result of times()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeTimesAdjoint","page":"API","title":"ComputationGraphs.NodeTimesAdjoint","text":"Node of a computation graph used to represent the result of timesAdjoint()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeTimesAdjointOnes","page":"API","title":"ComputationGraphs.NodeTimesAdjointOnes","text":"Node of a computation graph used to represent the result of timesAdjointOnes()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeUnitTimesAdjoint","page":"API","title":"ComputationGraphs.NodeUnitTimesAdjoint","text":"Node of a computation graph used to represent the result of unitTimesAdjoint()\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeVariable","page":"API","title":"ComputationGraphs.NodeVariable","text":"Node of a computation graph used to represent a variable whose value can be directly set. It is created by variable().\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#ComputationGraphs.NodeZeros","page":"API","title":"ComputationGraphs.NodeZeros","text":"Node of a computation graph used to represent a constant equal to an array of zeros. It is created by zeros().\n\n\n\n\n\n","category":"type"},{"location":"lib_public.html#Graph-evaluation","page":"API","title":"Graph evaluation","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.generateComputeFunctions","page":"API","title":"ComputationGraphs.generateComputeFunctions","text":"Generates a function that conditionally evaluates a node,      using closure & enforcing type stability.\n\nEach function will\n\ncheck if each parent need to be re-evaluated, if re-evaluates the parent and sets it's valid bit to true.\nalways recomputes the function\nwithout checking if it is needed (this should be checked by caller, to enable force=true)\nwithout setting the valid bit), which is expected to be set by the calling function. \n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.compute_node!","page":"API","title":"ComputationGraphs.compute_node!","text":"compute_node!(node)\ncompute_node!(graph,node)\ncompute_node!(graph,id)\n\nCall the function generated by generateComputeFunction that computes a single node.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.compute_with_ancestors!","page":"API","title":"ComputationGraphs.compute_with_ancestors!","text":"compute_with_ancestors!(node)\ncompute_with_ancestors!(graph,node)\ncompute_with_ancestors!(graph,id)\n\nCall the function generated by generateComputeFunction that computes a node and all its required parents.\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#Code-generation-2","page":"API","title":"Code generation","text":"","category":"section"},{"location":"lib_public.html#ComputationGraphs.nodes_str","page":"API","title":"ComputationGraphs.nodes_str","text":"Create initialization code\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.call_gs","page":"API","title":"ComputationGraphs.call_gs","text":"Create string to call function that does the computation\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.compute_str_recursive","page":"API","title":"ComputationGraphs.compute_str_recursive","text":"Create code to compute nodes (for gets) [recursive functions]\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.compute_str_unrolled","page":"API","title":"ComputationGraphs.compute_str_unrolled","text":"Create code to compute nodes (for gets) [single function with nested if's]\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#ComputationGraphs.compute_str_parallel","page":"API","title":"ComputationGraphs.compute_str_parallel","text":"Create parallel code to recompute all nodes\n\n\n\n\n\n","category":"function"},{"location":"lib_public.html#API-index","page":"API","title":"API index","text":"","category":"section"},{"location":"lib_public.html","page":"API","title":"API","text":"Pages = [\"lib_public.md\"]","category":"page"},{"location":"index.html#ComputationGraphs.jl","page":"Home","title":"ComputationGraphs.jl","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Making computation more efficient.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"A package for improving the speed (and energy consumption) of numerical computations that need to be performed repeatedly.","category":"page"},{"location":"index.html#Package-key-features","page":"Home","title":"Package key features","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Allocation-free operation\nRe-use of previously performed computations\nSymbolic differentiation\nAlgebraic simplifications","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"The User manual provides a tutorial explaining how to get started using ComputationGraphs.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Illustrative examples can be found on the Examples page.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"See the API for the complete list of documented functions and types.","category":"page"},{"location":"index.html#Documentation-outline","page":"Home","title":"Documentation outline","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Pages = [\n    \"man_installation.md\",\n    \"man_guide.md\",\n    \"man_differentiation.md\",\n    \"man_recipes.md\",\n    \"man_parallelization.md\",\n    \"man_code_generation.md\",\n    \"man_examples.md\",\n    \"lib_representation.md\",\n    \"lib_public.md\"\n]\nDepth = 1:2","category":"page"},{"location":"man_differentiation.html#Symbolic-differentiation","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"","category":"section"},{"location":"man_differentiation.html#Differentiation","page":"Symbolic differentiation","title":"Differentiation","text":"","category":"section"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"ComputationGraphs provides basic tools for symbolics differentiation. Not to be confused with automatic differentiation, which evaluates partial derivatives of a function as the function is being evaluated.","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"ComputationGraphs computes derivatives symbolically in three steps:","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"It operates on expressions encoded in the computation graph;\ncomputes the derivative of one node of the graph symbolically with respect to a given variable (using the standard calculus rules for differentiation); and\nthen dds the symbolic expression of the derivative to the computation graph.  ","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"Two points need emphasis:","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"The symbolic differentiation is performed when the graph is being built, rather than while the expressions are being evaluated as it is typically done in automatic differentiation.\nAs we shall see in Algebraic simplifications, this permits the discovery of \"simplification\" that can speed up computations.\nTypically, the derivative will reuse existing graph nodes (as we saw in the example in Building a computation graph). This also enables significant computational savings by reusing computation across the evaluation of a function and its derivative.","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"The function D is used to compute the partial derivative of an expression with respect to a variable; or more precisely, to augment the graph with the formula that computes the partial derivative.","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"The computation graph in Building a computation graph could have been generated using D as follows:","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"using ComputationGraphs\ngraph = ComputationGraph{Float64}()\nA = variable(graph, 4, 3)\nx = variable(graph, 3)\nb = variable(graph, 4)\ne = @add graph norm2(A*x-b)\ngrad = D(graph,e,x)","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"note: Note\nThe graph generated using D actually has a few extra nodes, but these nodes never really need to be computed due to Algebraic simplifications.","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"warning: Warning\nCurrently, ComputationGraphs only includes a relatively small set of rules for symbolic differentiation. These are expected to grow and the package matures.The set of rules currently used can be found in symbolic.pdf","category":"page"},{"location":"man_differentiation.html#Algebraic-simplifications","page":"Symbolic differentiation","title":"Algebraic simplifications","text":"","category":"section"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"ComputationGraphs include a small set of (very simple) rules that automatically simplify a computation graph at the time it is being creates. These include","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"The sum of any expression with a scalar/vector/matrix zero, does not change the value of the expression.\nAny product with a zero scalar/vector/matrix is always zero.\nThe adjoint of an adjoint returns the original expression.\nMultiplication of a matrix by a vector of ones, corresponds to replacing each row of the matrix by the sum of its elements.\nMultiplication of a matrix by the k-th vector of a canonical basis, extracts the k-th column of the matrix.\nEtc.","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"These rules are applied when the graph is being built to \"reduce\" the graph. For example, if we try to build a graph for the expression y = 0 * a + b, we actually end up with the graph of y = b. To be precise, the graph will actually have nodes for 0 and a, but y is obtained directly from b without any multiplication and addition.","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"It might seem that such rules are too simple to be useful, as no one would ever try to encode into a computation graph the expression 0 * a. However, such computations actually arise very often in differentiation. For example, when applying the product rule to compute the derivate of a * x + b with respect to x, we get","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"beginalign*\nnabla_x (a x + b) = a (nabla_x x) + (nabla_x a) x + nabla_x b\n=a times 1 + 0 times x + 0\n=a\nendalign*","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"where the last equality results precisely from applying the very simple rules listed above. Because these rules are applied when the graph is being built it is immediately discovered that evaluating this partial derivative actually does not require any computation.","category":"page"},{"location":"man_differentiation.html","page":"Symbolic differentiation","title":"Symbolic differentiation","text":"warning: Warning\nCurrently, ComputationGraphs only includes a very small set of rules for symbolic simplification. These are expected to grow and the package matures.When trying to different an expression not supported by the current set of rules a DomainError exception is thrown.","category":"page"}]
}
